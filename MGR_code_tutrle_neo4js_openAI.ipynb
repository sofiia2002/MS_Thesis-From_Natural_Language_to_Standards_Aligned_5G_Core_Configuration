{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvefgy8IBgTU"
      },
      "outputs": [],
      "source": [
        "OPEN_AI_SECRET_KEY=\"example\"\n",
        "\n",
        "NEO4J_URI=\"neo4j+s://example.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"example\"\n",
        "AURA_INSTANCEID=\"example\"\n",
        "AURA_INSTANCENAME=\"Instance\"\n",
        "\n",
        "NEO4J_DB = \"neo4j\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing turtle file into AuraDB"
      ],
      "metadata": {
        "id": "iMYYhbh8xPbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install rdflib rdflib-neo4j"
      ],
      "metadata": {
        "id": "Dcyqxv4uyWY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing ttl file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "ttl_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "v48xb2ke4vSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest Turtle into Aura (or any Neo4j) with RDFLib-Neo4j\n",
        "from rdflib import Graph\n",
        "from rdflib_neo4j import Neo4jStore, Neo4jStoreConfig, HANDLE_VOCAB_URI_STRATEGY\n",
        "\n",
        "config = Neo4jStoreConfig(\n",
        "    auth_data={\"uri\": NEO4J_URI, \"database\": NEO4J_DB, \"user\": NEO4J_USERNAME, \"pwd\": NEO4J_PASSWORD},\n",
        "    handle_vocab_uri_strategy=HANDLE_VOCAB_URI_STRATEGY.IGNORE,  # or SHORTEN/MAP/KEEP\n",
        "    batching=True,  # optional: buffer writes for speed; remember to close()\n",
        ")\n",
        "\n",
        "g = Graph(store=Neo4jStore(config=config))\n",
        "\n",
        "# Auto-create the required uniqueness constraint if missing:\n",
        "g.open(configuration=None, create=True)  # creates CONSTRAINT on :Resource(uri)\n",
        "\n",
        "# Parse from local file:\n",
        "g.parse(ttl_path, format=\"turtle\")       # or format=\"ttl\"\n",
        "\n",
        "# Or parse directly from a URL instead:\n",
        "# g.parse(\"https://example.com/your.ttl\", format=\"turtle\")\n",
        "\n",
        "print(\"Triples loaded into Neo4j via RDFLib:\", len(g))\n",
        "g.close(True)  # commit pending buffered writes\n"
      ],
      "metadata": {
        "id": "sEvxbSWqyFES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding usage of LangChain"
      ],
      "metadata": {
        "id": "RBm4jStx-uQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install modern LangChain split packages\n",
        "!pip -q install langchain langchain-community langchain-openai langchain-neo4j neo4j"
      ],
      "metadata": {
        "id": "3ue3TXID6kBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_SECRET_KEY"
      ],
      "metadata": {
        "id": "i5u-37Hv_VX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_MODEL = \"text-embedding-3-large\"   # or \"text-embedding-3-small\"\n",
        "CHAT_MODEL  = \"gpt-5\"              # or \"gpt-4o\""
      ],
      "metadata": {
        "id": "n55mqUwpGEo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "with driver.session() as s:\n",
        "    result = s.run(\"RETURN 1 AS ok\").single()\n",
        "print(\"Neo4j OK:\", result[\"ok\"] == 1)"
      ],
      "metadata": {
        "id": "_1RfNQSfGMU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_neo4j import Neo4jVector\n",
        "\n",
        "emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "# configure your label + text properties here\n",
        "DOC_NODE_LABEL = \"Document\"                   # <-- change to your label\n",
        "TEXT_PROPS     = [\"title\", \"content\"]         # <-- change to your text fields\n",
        "INDEX_NAME     = \"docs_embedding\"             # Neo4j vector index name\n",
        "EMB_PROP       = \"embedding\"                  # property to store vectors\n",
        "KEYWORD_INDEX  = \"docs_keyword\"               # optional: for hybrid search\n",
        "\n",
        "vstore = Neo4jVector.from_existing_graph(\n",
        "    embedding=emb,\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD,\n",
        "    node_label=DOC_NODE_LABEL,\n",
        "    text_node_properties=TEXT_PROPS,\n",
        "    embedding_node_property=EMB_PROP,\n",
        "    index_name=INDEX_NAME,\n",
        "    keyword_index_name=KEYWORD_INDEX,   # keep if you want hybrid search\n",
        "    search_type=\"hybrid\"                # or \"vector\"\n",
        ")\n",
        "\n",
        "print(\"Vector store ready (existing graph).\")"
      ],
      "metadata": {
        "id": "Qqj1AepPGNhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_neo4j import Neo4jVector\n",
        "\n",
        "def load_graph_chunks(limit=8000):\n",
        "    \"\"\"\n",
        "    Robust chunk builder: avoids missing property warnings\n",
        "    and guarantees non-null strings for page_content.\n",
        "    Adjust the property candidates to match your schema.\n",
        "    \"\"\"\n",
        "    q = \"\"\"\n",
        "    MATCH (a)-[r]->(b)\n",
        "    WITH\n",
        "      coalesce(\n",
        "        a.name, a.id, a.title, a.label, a.shortName, a.code, elementId(a)\n",
        "      ) AS left_any,\n",
        "      head(labels(a)) AS left_label,\n",
        "      type(r) AS rel,\n",
        "      coalesce(\n",
        "        r.name, r.id, r.protocol, r.role, ''   // add r.interface here if you *do* have it\n",
        "      ) AS r_any,\n",
        "      coalesce(\n",
        "        b.name, b.id, b.title, b.label, b.shortName, b.code, elementId(b)\n",
        "      ) AS right_any,\n",
        "      head(labels(b)) AS right_label\n",
        "    WITH\n",
        "      toString(left_any)  AS left,\n",
        "      coalesce(left_label,'?')   AS left_label,\n",
        "      rel,\n",
        "      toString(r_any)     AS rname,\n",
        "      toString(right_any) AS right,\n",
        "      coalesce(right_label,'?')  AS right_label\n",
        "    RETURN\n",
        "      'Left: ' + left + ' (' + left_label + ')' +\n",
        "      '\\nRel: ' + rel +\n",
        "      CASE WHEN rname IS NOT NULL AND rname <> '' THEN ' [' + rname + ']' ELSE '' END +\n",
        "      '\\nRight: ' + right + ' (' + right_label + ')' AS text,\n",
        "      {left:left, right:right, rel:rel, rname:rname, left_label:left_label, right_label:right_label} AS meta\n",
        "    LIMIT $limit\n",
        "    \"\"\"\n",
        "    with driver.session() as s:\n",
        "        rows = s.run(q, limit=limit).data()\n",
        "\n",
        "    # Defensive: keep only rows that produced text\n",
        "    docs = [Document(page_content=r.get(\"text\", \"\"), metadata=r.get(\"meta\", {}))\n",
        "            for r in rows if r.get(\"text\")]\n",
        "    return docs\n",
        "\n",
        "docs = load_graph_chunks(limit=8000)\n",
        "print(f\"Built {len(docs)} chunk(s).\")\n",
        "if docs:\n",
        "    print(\"Example:\\n\", docs[0].page_content[:300])\n",
        "\n",
        "emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "vstore = Neo4jVector.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=emb,\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD,\n",
        "    # customize storage / indexes:\n",
        "    node_label=\"Chunk\",\n",
        "    text_node_property=\"text\",        # <-- singular\n",
        "    embedding_node_property=\"embedding\",\n",
        "    index_name=\"chunk_embedding\",\n",
        "    search_type=\"hybrid\",             # creates vector + keyword indexes\n",
        "    keyword_index_name=\"chunk_keyword\"\n",
        ")\n",
        "\n",
        "print(\"Vector store ready (chunks from triples).\")"
      ],
      "metadata": {
        "id": "NBaF75q8G5xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
        "\n",
        "# hybrid works well for acronyms (AMF/SMF/N2/N11/5QI); fallback to \"similarity\" if needed\n",
        "try:\n",
        "    retriever = vstore.as_retriever(search_type=\"hybrid\", search_kwargs={\"k\": 6})\n",
        "except Exception:\n",
        "    retriever = vstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "\n",
        "TELECOM_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"You are a 5G/Open5GS expert. Your purpose is to translate high-level requirements\n",
        "    for a 5GCore network into a YANG configuration, if in the intent user asks for a\n",
        "    configuration. Analyze carefully all of the requirements for a YANG format and keep\n",
        "    it strict. Ensure that YANG format follows all the rules to be validated successfully\n",
        "    by libyang tool.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "At the end, list bullet 'evidence lines' in the format (Left|Rel|Right).\"\"\"\n",
        ")\n",
        "\n",
        "# TELECOM_PROMPT = PromptTemplate.from_template(\n",
        "#     \"\"\"You are a 5G/Open5GS expert. Use standards-aware, precise language.\n",
        "# Expand acronyms on first use (e.g., AMF—Access and Mobility Management Function).\n",
        "# If interfaces appear, mention their purpose (e.g., N11—AMF↔SMF, session control).\n",
        "\n",
        "# Question:\n",
        "# {question}\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Answer clearly. At the end, list 2–5 bullet 'evidence lines' in the format (Left|Rel|Right).\"\"\"\n",
        "# )\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": TELECOM_PROMPT},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "query = \"Enable each network function to declare the services it exposes, including version and supported protocols.\"\n",
        "resp = qa({\"query\": query})\n",
        "\n",
        "print(\"Q:\", query, \"\\n\")\n",
        "print(resp[\"result\"])\n",
        "print(\"\\nSources:\")\n",
        "for d in resp[\"source_documents\"][:5]:\n",
        "    print(\"-\", (d.page_content.splitlines() or [d.page_content[:120]])[0])\n"
      ],
      "metadata": {
        "id": "VHKXIBZZiyyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flow for comparing doc loading and knowledge graph code"
      ],
      "metadata": {
        "id": "gJOBNa9V8ED6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community faiss-cpu pypdf"
      ],
      "metadata": {
        "id": "hipJPkQa8Dkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, glob, textwrap\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Reuse existing settings if you have them, else set defaults\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "assert OPENAI_API_KEY, \"Set OPENAI_API_KEY first.\"\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"text-embedding-3-small\")  # good default for speed/cost\n",
        "CHAT_MODEL  = os.getenv(\"CHAT_MODEL\",  \"gpt-4o-mini\")\n",
        "\n",
        "# If you already created vstore/qa for Neo4j earlier, we’ll reuse them."
      ],
      "metadata": {
        "id": "GbSfsns-8LcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_DIR = \"./pdfs\"     # <-- point to your folder of PDFs\n",
        "PDF_GLOB = \"*.pdf\"\n",
        "\n",
        "# 3a) Load PDFs\n",
        "pdf_files = sorted(glob.glob(os.path.join(PDF_DIR, PDF_GLOB)))\n",
        "assert pdf_files, f\"No PDFs found under {PDF_DIR}. Put some files there or change PDF_DIR.\"\n",
        "\n",
        "raw_docs = []\n",
        "for f in pdf_files:\n",
        "    try:\n",
        "        loader = PyPDFLoader(f)\n",
        "        raw_docs.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: failed to load {f}: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(raw_docs)} pages from {len(pdf_files)} PDF(s).\")\n",
        "\n",
        "# 3b) Chunking\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "pdf_docs = splitter.split_documents(raw_docs)\n",
        "# add simple, consistent metadata for debugging\n",
        "for d in pdf_docs:\n",
        "    d.metadata[\"source_file\"] = d.metadata.get(\"source\") or d.metadata.get(\"file_path\")\n",
        "    d.metadata[\"page\"] = d.metadata.get(\"page\", d.metadata.get(\"page_number\"))\n",
        "\n",
        "print(f\"Chunks: {len(pdf_docs)}\")\n",
        "\n",
        "# 3c) Embeddings + FAISS vector store\n",
        "emb_pdf = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "pdf_vstore = FAISS.from_documents(pdf_docs, emb_pdf)\n",
        "\n",
        "# Retriever for PDF\n",
        "pdf_retriever = pdf_vstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "print(\"PDF vector store ready.\")"
      ],
      "metadata": {
        "id": "1WbksS-M8VAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neo4j retriever/qa: try to reuse if already defined; else raise a helpful error\n",
        "try:\n",
        "    retriever_neo4j = vstore.as_retriever(\n",
        "        search_type=\"similarity\",                 # <- allowed by VectorStoreRetriever\n",
        "        search_kwargs={\"k\": 6, \"search_type\": \"hybrid\"}  # <- passed through to Neo4jVector\n",
        "    )\n",
        "except Exception:\n",
        "    # fallback to pure vector if hybrid isn't available (e.g., no keyword index)\n",
        "    retriever_neo4j = vstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "\n",
        "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
        "\n",
        "TELECOM_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"You are a 5G/Open5GS expert. Your purpose is to translate high-level requirements\n",
        "    for a 5GCore network into a YANG configuration. Analyze carefully all of the requirements\n",
        "    for a YANG format and keep it strict. Ensure that YANG format follows all the rules to be\n",
        "    validated successfully by libyang tool.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "At the end, list bullet 'evidence lines' in the format (Left|Rel|Right).\"\"\"\n",
        ")\n",
        "\n",
        "qa_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=pdf_retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": TELECOM_PROMPT},\n",
        "    return_source_documents=True,\n",
        ")\n",
        "\n",
        "qa_neo4j = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever_neo4j,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": TELECOM_PROMPT},\n",
        "    return_source_documents=True,\n",
        ")\n",
        "\n",
        "print(\"RAG chains ready (Neo4j + PDF).\")"
      ],
      "metadata": {
        "id": "Srb68T---25C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RunResult:\n",
        "    pipeline: str\n",
        "    answer: str\n",
        "    time_s: float\n",
        "    sources: List[Dict[str, Any]]\n",
        "\n",
        "def _summarize_sources(sources: List[Any], kind: str) -> List[str]:\n",
        "    out = []\n",
        "    if kind == \"pdf\":\n",
        "        for d in sources[:6]:\n",
        "            src = d.metadata.get(\"source_file\", \"unknown.pdf\")\n",
        "            page = d.metadata.get(\"page\", \"?\")\n",
        "            first = (d.page_content or \"\").strip().replace(\"\\n\", \" \")\n",
        "            out.append(f\"{os.path.basename(src)}:p{page} — {first[:160]}{'…' if len(first)>160 else ''}\")\n",
        "    else:  # neo4j\n",
        "        for d in sources[:6]:\n",
        "            first = (d.page_content or \"\").strip().splitlines()\n",
        "            first_line = first[0] if first else \"\"\n",
        "            out.append(first_line[:180] + (\"…\" if len(first_line) > 180 else \"\"))\n",
        "    return out\n",
        "\n",
        "def ask_both(question: str, judge: bool = True) -> Dict[str, Any]:\n",
        "    # Neo4j\n",
        "    t0 = time.time()\n",
        "    neo = qa_neo4j({\"query\": question})\n",
        "    neo_t = time.time() - t0\n",
        "\n",
        "    # PDF\n",
        "    t1 = time.time()\n",
        "    pdf = qa_pdf({\"query\": question})\n",
        "    pdf_t = time.time() - t1\n",
        "\n",
        "    neo_sources = _summarize_sources(neo[\"source_documents\"], \"neo4j\")\n",
        "    pdf_sources = _summarize_sources(pdf[\"source_documents\"], \"pdf\")\n",
        "\n",
        "    result = {\n",
        "        \"neo4j\": RunResult(\"neo4j\", neo[\"result\"], neo_t, neo_sources),\n",
        "        \"pdf\":   RunResult(\"pdf\",   pdf[\"result\"], pdf_t, pdf_sources)\n",
        "    }\n",
        "\n",
        "    if not judge:\n",
        "        return result\n",
        "\n",
        "    # LLM-as-judge: which is more correct/grounded *given the retrieved contexts*?\n",
        "    judge_prompt = PromptTemplate.from_template(\n",
        "        \"\"\"You are grading two answers to the same question using only the provided contexts.\n",
        "Score on: factual correctness, grounding to context, telecom clarity, YANG format correctness.\n",
        "\n",
        "For both you are a 5G/Open5GS expert. Your purpose is to translate high-level requirements\n",
        "for a 5GCore network into a YANG configuration. Analyze carefully all of the requirements\n",
        "for a YANG format and keep it strict. Ensure that YANG format follows all the rules to be\n",
        "validated successfully by libyang tool. Answer should include a YANG template and a short\n",
        "description of the result.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer A (Neo4j):\n",
        "{ans_a}\n",
        "\n",
        "Context A (Neo4j top sources):\n",
        "{ctx_a}\n",
        "\n",
        "Answer B (PDF):\n",
        "{ans_b}\n",
        "\n",
        "Context B (PDF top sources):\n",
        "{ctx_b}\n",
        "\n",
        "Respond as JSON with fields:\n",
        "- \"winner\": one of [\"neo4j\",\"pdf\",\"tie\"]\n",
        "- \"rationale\": 1-3 sentences explaining the decision.\n",
        "\"\"\"\n",
        "    )\n",
        "    judge_llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
        "    judge_in = judge_prompt.format(\n",
        "        question=question,\n",
        "        ans_a=result[\"neo4j\"].answer,\n",
        "        ctx_a=\"\\n\".join(result[\"neo4j\"].sources),\n",
        "        ans_b=result[\"pdf\"].answer,\n",
        "        ctx_b=\"\\n\".join(result[\"pdf\"].sources),\n",
        "    )\n",
        "    j = judge_llm.invoke(judge_in).content\n",
        "    result[\"judge_raw\"] = j\n",
        "    return result"
      ],
      "metadata": {
        "id": "NaUrPtdR-4fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def show_comparison(question: str, judge: bool = True):\n",
        "    res = ask_both(question, judge=judge)\n",
        "\n",
        "    neo = res[\"neo4j\"]\n",
        "    pdf = res[\"pdf\"]\n",
        "\n",
        "    print(\"=\"*90)\n",
        "    print(\"QUESTION:\")\n",
        "    print(question)\n",
        "    print(\"=\"*90)\n",
        "    print(\"[Neo4j] time: %.2fs\" % neo.time_s)\n",
        "    print(textwrap.fill(neo.answer, width=100))\n",
        "    print(\"\\nSources (Neo4j):\")\n",
        "    for s in neo.sources:\n",
        "        print(\" •\", s)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*90 + \"\\n\")\n",
        "\n",
        "    print(\"[PDF]   time: %.2fs\" % pdf.time_s)\n",
        "    print(textwrap.fill(pdf.answer, width=100))\n",
        "    print(\"\\nSources (PDF):\")\n",
        "    for s in pdf.sources:\n",
        "        print(\" •\", s)\n",
        "\n",
        "    if judge and \"judge_raw\" in res:\n",
        "        print(\"\\n\" + \"=\"*90)\n",
        "        print(\"LLM JUDGE:\")\n",
        "        try:\n",
        "            j = json.loads(res[\"judge_raw\"])\n",
        "        except Exception:\n",
        "            print(res[\"judge_raw\"])\n",
        "        else:\n",
        "            print(\"Winner:\", j.get(\"winner\"))\n",
        "            print(\"Rationale:\", j.get(\"rationale\"))\n",
        "\n",
        "# Example:\n",
        "# show_comparison(\"Which network function coordinates UE registration and which interfaces are involved?\")"
      ],
      "metadata": {
        "id": "LGYBbT_5-6kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def max_grounding_similarity(answer: str, retrieved_docs: List[Any], emb: OpenAIEmbeddings) -> float:\n",
        "    ans_vec = np.array(emb.embed_query(answer), dtype=float)\n",
        "    ctx_vecs = np.array([emb.embed_query(d.page_content[:1000]) for d in retrieved_docs], dtype=float)\n",
        "    # cosine similarity\n",
        "    sims = (ctx_vecs @ ans_vec) / (np.linalg.norm(ctx_vecs, axis=1) * np.linalg.norm(ans_vec) + 1e-9)\n",
        "    return float(np.max(sims)) if len(sims) else 0.0\n",
        "\n",
        "def compare_with_scores(question: str):\n",
        "    res = ask_both(question, judge=False)\n",
        "    neo = res[\"neo4j\"]; pdf = res[\"pdf\"]\n",
        "\n",
        "    # reuse the same embedder model used for PDF; it’s fine for a relative score\n",
        "    emb_eval = emb_pdf\n",
        "\n",
        "    # We need the full docs, not the summarized strings → rerun each quickly to capture docs\n",
        "    neo_full = qa_neo4j({\"query\": question})\n",
        "    pdf_full = qa_pdf({\"query\": question})\n",
        "\n",
        "    neo_score = max_grounding_similarity(neo_full[\"result\"], neo_full[\"source_documents\"], emb_eval)\n",
        "    pdf_score = max_grounding_similarity(pdf_full[\"result\"], pdf_full[\"source_documents\"], emb_eval)\n",
        "\n",
        "    print(f\"Grounding similarity (cosine to retrieved context):\")\n",
        "    print(f\"  Neo4j: {neo_score:.3f}\")\n",
        "    print(f\"  PDF  : {pdf_score:.3f}\")\n",
        "    return {\"neo4j\": neo_score, \"pdf\": pdf_score}\n",
        "\n",
        "# Example:\n",
        "# compare_with_scores(\"Explain the UE registration flow and the NFs involved.\")"
      ],
      "metadata": {
        "id": "ARREqcF6-9rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_with_scores(\"Enable each network function to declare the services it exposes, including version and supported protocols.\")"
      ],
      "metadata": {
        "id": "1kC7Cy7dDBU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " show_comparison(\"Enable each network function to declare the services it exposes, including version and supported protocols.\")"
      ],
      "metadata": {
        "id": "gGLTRtI6DOxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "​​import pandas as pd\n",
        "\n",
        "# --- Step 1: Select a subset of queries for these tests ---\n",
        "ablation_queries = {\n",
        "    \"Q2\": \"How does the SMF interact with the UPF to enforce a QoS policy?\",\n",
        "    \"Q7\": \"Define a network slice for eMBB services, identified by S-NSSAI value 1, and enforce a maximum slice data rate of 500 Mbps for downlink and 50 Mbps for uplink.\",\n",
        "    \"Q9\": \"Configure the SMF to select a specific UPF for all traffic associated with the Data Network Name (DNN) 'internet'. The PDU session should be type IPv4.\"\n",
        "}\n",
        "\n",
        "# --- Step 2: Define the parameters for each study ---\n",
        "chunk_size_options = [512, 1024, 2048]\n",
        "embedding_model_options = [\"text-embedding-3-small\", \"text-embedding-3-large\"]\n",
        "k_value_options = [3, 6, 10]\n",
        "\n",
        "# --- Step 3: Create a place to store all results ---\n",
        "ablation_results = []\n",
        "\n",
        "# --- Step 4: Run the experiments ---\n",
        "\n",
        "# == STUDY 1: CHUNK SIZE (PDF-only RAG) ==\n",
        "print(\"=\"*20, \"STARTING: Chunk Size Ablation\", \"=\"*20)\n",
        "for size in chunk_size_options:\n",
        "    print(f\"\\\\n--- Testing Chunk Size: {size} ---\")\n",
        "    # 1. Re-build the PDF pipeline with the new chunk size\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=int(size*0.2))\n",
        "    pdf_docs_abl = splitter.split_documents(raw_docs) # Assumes `raw_docs` is loaded from a previous cell\n",
        "    pdf_vstore_abl = FAISS.from_documents(pdf_docs_abl, emb_pdf) # Assumes `emb_pdf` is defined\n",
        "    pdf_retriever_abl = pdf_vstore_abl.as_retriever(search_kwargs={\"k\": 6}) # Keep k constant\n",
        "    qa_pdf_abl = RetrievalQA.from_chain_type(llm=llm, retriever=pdf_retriever_abl, chain_type=\"stuff\", return_source_documents=True)\n",
        "\n",
        "    # 2. Run queries and collect results\n",
        "    for q_id, query in ablation_queries.items():\n",
        "        t_start = time.time()\n",
        "        result = qa_pdf_abl({\"query\": query})\n",
        "        latency = time.time() - t_start\n",
        "        grounding_score = max_grounding_similarity(result[\"result\"], result[\"source_documents\"], emb_pdf)\n",
        "\n",
        "        ablation_results.append({\n",
        "            \"study\": \"Chunk Size\",\n",
        "            \"parameter\": size,\n",
        "            \"query_id\": q_id,\n",
        "            \"latency_s\": latency,\n",
        "            \"grounding_score\": grounding_score\n",
        "        })\n",
        "        print(f\"Query {q_id} with chunk size {size}: Grounding = {grounding_score:.3f}, Latency = {latency:.2f}s\")\n",
        "\n",
        "\n",
        "# == STUDY 2: EMBEDDING MODEL (Both pipelines) ==\n",
        "print(\"\\\\n\"+\"=\"*20, \"STARTING: Embedding Model Ablation\", \"=\"*20)\n",
        "for model_name in embedding_model_options:\n",
        "    print(f\"\\\\n--- Testing Embedding Model: {model_name} ---\")\n",
        "    # 1. Re-initialize the embedding model and rebuild stores\n",
        "    emb_abl = OpenAIEmbeddings(model=model_name)\n",
        "\n",
        "    # PDF Pipeline\n",
        "    pdf_vstore_abl = FAISS.from_documents(pdf_docs, emb_abl) # Assumes `pdf_docs` is defined\n",
        "    pdf_retriever_abl = pdf_vstore_abl.as_retriever(search_kwargs={\"k\": 6})\n",
        "    qa_pdf_abl = RetrievalQA.from_chain_type(llm=llm, retriever=pdf_retriever_abl, chain_type=\"stuff\", return_source_documents=True)\n",
        "\n",
        "    # Graph Pipeline (Note: In a real scenario, you'd re-embed and re-index Neo4j, which is slow.\n",
        "    # For this test, we can simulate by just changing the query embedder if the retriever supports it.\n",
        "    # Here we will just re-create the retriever with the new embedding function for the query.)\n",
        "    # This is a simplification; a full test would require re-indexing Neo4j.\n",
        "    # For now, we will focus on the PDF pipeline for this ablation as it's easier to demonstrate.\n",
        "\n",
        "    # 2. Run queries on the PDF pipeline and collect results\n",
        "    for q_id, query in ablation_queries.items():\n",
        "        t_start = time.time()\n",
        "        result = qa_pdf_abl({\"query\": query})\n",
        "        latency = time.time() - t_start\n",
        "        grounding_score = max_grounding_similarity(result[\"result\"], result[\"source_documents\"], emb_abl)\n",
        "\n",
        "        ablation_results.append({\n",
        "            \"study\": \"Embedding Model\",\n",
        "            \"parameter\": model_name,\n",
        "            \"query_id\": q_id,\n",
        "            \"latency_s\": latency,\n",
        "            \"grounding_score\": grounding_score\n",
        "        })\n",
        "        print(f\"Query {q_id} with model {model_name}: Grounding = {grounding_score:.3f}, Latency = {latency:.2f}s\")\n",
        "\n",
        "\n",
        "# == STUDY 3: RETRIEVAL K VALUE (Both pipelines) ==\n",
        "print(\"\\\\n\"+\"=\"*20, \"STARTING: Retrieval K Value Ablation\", \"=\"*20)\n",
        "for k in k_value_options:\n",
        "    print(f\"\\\\n--- Testing k = {k} ---\")\n",
        "    # 1. Re-build retrievers with the new k value\n",
        "    pdf_retriever_abl = pdf_vstore.as_retriever(search_kwargs={\"k\": k})\n",
        "    qa_pdf_abl = RetrievalQA.from_chain_type(llm=llm, retriever=pdf_retriever_abl, chain_type=\"stuff\", return_source_documents=True)\n",
        "\n",
        "    retriever_neo4j_abl = vstore.as_retriever(search_type=\"hybrid\", search_kwargs={\"k\": k})\n",
        "    qa_neo4j_abl = RetrievalQA.from_chain_type(llm=llm, retriever=retriever_neo4j_abl, chain_type=\"stuff\", return_source_documents=True)\n",
        "\n",
        "    # 2. Run queries on both pipelines and collect results\n",
        "    for q_id, query in ablation_queries.items():\n",
        "        # PDF\n",
        "        t_start_pdf = time.time()\n",
        "        result_pdf = qa_pdf_abl({\"query\": query})\n",
        "        latency_pdf = time.time() - t_start_pdf\n",
        "        grounding_pdf = max_grounding_similarity(result_pdf[\"result\"], result_pdf[\"source_documents\"], emb_pdf)\n",
        "        ablation_results.append({\"study\": f\"K Value (PDF)\", \"parameter\": k, \"query_id\": q_id, \"latency_s\": latency_pdf, \"grounding_score\": grounding_pdf})\n",
        "\n",
        "        # Graph\n",
        "        t_start_neo = time.time()\n",
        "        result_neo = qa_neo4j_abl({\"query\": query})\n",
        "        latency_neo = time.time() - t_start_neo\n",
        "        grounding_neo = max_grounding_similarity(result_neo[\"result\"], result_neo[\"source_documents\"], emb_pdf)\n",
        "        ablation_results.append({\"study\": f\"K Value (Graph)\", \"parameter\": k, \"query_id\": q_id, \"latency_s\": latency_neo, \"grounding_score\": grounding_neo})\n",
        "\n",
        "        print(f\"Query {q_id} with k={k}: PDF Grounding={grounding_pdf:.3f}, Graph Grounding={grounding_neo:.3f}\")\n",
        "\n",
        "# --- Step 5: Document the results ---\n",
        "df_results = pd.DataFrame(ablation_results)\n",
        "print(\"\\\\n\\\\n--- ABLATION STUDY RESULTS ---\")\n",
        "print(df_results.to_markdown(index=False))\n"
      ],
      "metadata": {
        "id": "Cav5ImFdWs62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}