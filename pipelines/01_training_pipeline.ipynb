{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ff309e4",
      "metadata": {
        "id": "6ff309e4"
      },
      "source": [
        "# 01 — Training Pipeline (Adapted to Your CSVs)\n",
        "\n",
        "This notebook is **tailored to the retrieved files**:\n",
        "\n",
        "- **`./data/decompose.csv`** → Step 2 (complex → simple). We **explode** each row so that a *Summary sentence* maps to **multiple** simple *Factoids*.\n",
        "- **`./data/triples.csv`** → Step 3 (simple → triple). We build targets in the format **`Object 1 | Relationship | Object 2`**.\n",
        "- **`./data/reltype.csv`** → Step 4a (relation normalization). We create a training set of **raw relation strings** (from `triples.csv`) mapped to **canonical relation labels** (from `reltype.csv`) using **exact+fuzzy matching**.\n",
        "- **`./data/classes.csv`** → Step 4b (class taxonomy mapping). We build a **taxonomy** from the hierarchical columns and map each **Entity** to a **leaf node**. The notebook emits a `taxonomy.csv` that the ingestion notebook uses.\n",
        "\n",
        "We keep models small and finetune-ready:\n",
        "- **T5-small** for Steps 2 & 3 (seq2seq)\n",
        "- **DistilBERT** for Steps 4a & 4b (classification)\n",
        "\n",
        "> Training is commented out by default, so it should be uncommented to run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91aae7dd",
      "metadata": {
        "id": "91aae7dd"
      },
      "source": [
        "## 0) Environment Setup\n",
        "\n",
        "If needed, install packages. If you're offline here, make sure the environment already has them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45c89ceb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45c89ceb",
        "outputId": "198af9b1-e757-4dbb-d03f-f6d4edfcb725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting python-levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting Levenshtein==0.27.1 (from python-levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-levenshtein\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [python-levenshtein]\n",
            "\u001b[1A\u001b[2KSuccessfully installed Levenshtein-0.27.1 python-levenshtein-0.27.1 rapidfuzz-3.14.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from rdflib) (3.2.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdflib\n",
            "Successfully installed rdflib-7.1.4\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# !pip install -U pip\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "# !pip install transformers datasets accelerate sentencepiece scikit-learn evaluate\n",
        "# !pip install pandas numpy tqdm matplotlib python-levenshtein rapidfuzz\n",
        "# !pip install nltk spacy rdflib joblib\n",
        "# !python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "282619a6",
      "metadata": {
        "id": "282619a6"
      },
      "source": [
        "## 1) Config & Paths\n",
        "\n",
        "We point directly at the CSVs uploaded under `./data/`. Outputs go to `./trained_models/` within this working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7df0d1ff",
      "metadata": {
        "id": "7df0d1ff"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "DATA_DIR = Path(\"./data\")\n",
        "\n",
        "CSV_DECOMPOSE = DATA_DIR / \"decompose.csv\"   # Source, Summary sentence, Factoids\n",
        "CSV_TRIPLES   = DATA_DIR / \"triples.csv\"     # Factoid, Triplet, Object 1, Relationship, Object 2\n",
        "CSV_RELTYPE   = DATA_DIR / \"reltype.csv\"     # relationship, type, domain, range, otherCharacteristics\n",
        "CSV_CLASSES   = DATA_DIR / \"classes.csv\"     # Class, Parent Class\n",
        "\n",
        "OUT_DIR = Path(\"./drive/MyDrive/MGR/trained_models\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_DIR_SIMPLIFIER   = OUT_DIR / \"t5_simplifier_step2\"\n",
        "MODEL_DIR_TRIPLE       = OUT_DIR / \"t5_triple_step3\"\n",
        "MODEL_DIR_REL_CLASSIF  = OUT_DIR / \"distilbert_relation_step4a\"\n",
        "MODEL_DIR_CLASS_CLASSIF= OUT_DIR / \"distilbert_class_step4b\"\n",
        "\n",
        "LABELMAP_DIR = OUT_DIR / \"label_maps\"\n",
        "LABELMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_OUT = Path(\"./data\")\n",
        "DATA_OUT.mkdir(parents=True, exist_ok=True)\n",
        "TAXONOMY_CSV = DATA_OUT / \"taxonomy.csv\"\n",
        "\n",
        "BASE_NS = \"http://example.org/telecom#\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45ed6f3",
      "metadata": {
        "id": "d45ed6f3"
      },
      "source": [
        "## 2) Inspect & Parse Your CSVs\n",
        "\n",
        "We auto-parse & normalize columns from four files:\n",
        "- `decompose.csv` → pairs: **(Summary sentence → individual factoid)**\n",
        "- `triples.csv` → pairs: **(Simple sentence → `Object 1 | Relationship | Object 2`)**\n",
        "- `reltype.csv` → canonical **relation vocabulary** (with `domain`, `range`, `type`)\n",
        "- `classes.csv` → hierarchical **class taxonomy** (we emit `taxonomy.csv` for later use)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a8fbb093",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8fbb093",
        "outputId": "f9c2a7fd-7005-4a62-9880-ea6853abdd00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decompose.csv columns: ['Source', 'Summary sentence', 'Factoids'] shape: (612, 3)\n",
            "triples.csv   columns: ['Factoid', 'Triplet', 'Object 1', 'Relationship', 'Object 2', 'Unnamed: 5'] shape: (1454, 6)\n",
            "reltype.csv   columns: ['relationship', 'type', 'domain', 'range', 'otherCharacteristics'] shape: (506, 5)\n",
            "classes.csv   columns: ['Class', 'Parent Class'] shape: (1894, 2)\n",
            "Step2 pairs (complex→simple): (2349, 2)\n",
            "                                             complex  \\\n",
            "0  The 5G Core Network introduced in 3GPP Release...   \n",
            "1  The 5G Core Network introduced in 3GPP Release...   \n",
            "2  The 5G Core Network introduced in 3GPP Release...   \n",
            "3  The 5G Core Network introduced in 3GPP Release...   \n",
            "4  The 5G Core Network introduced in 3GPP Release...   \n",
            "\n",
            "                                             simple  \n",
            "0  5G Core Network is introduced in 3GPP Release 15  \n",
            "1           5G Core Network adopts architecture SBA  \n",
            "2                  Network Function exposes Service  \n",
            "3      Service is available via API (HTTP/2 + JSON)  \n",
            "4                          API uses protocol HTTP/2  \n",
            "Step3 pairs (simple→triple_text): (1454, 2)\n",
            "                                              simple  \\\n",
            "0  5G Core Network is a introduced in 3GPP Releas...   \n",
            "1  Service-Based Architecture (SBA) enables dynam...   \n",
            "2               SBA model decouples service consumer   \n",
            "3  Service consumer is decoupled from service pro...   \n",
            "4  SBA model achieves decoupling via Network Repo...   \n",
            "\n",
            "                                         triple_text  \n",
            "0     5GCoreNetwork | isIntroducedIn | 3GPPRelease15  \n",
            "1  ServiceBasedArchitectureSBA | enables | Dynami...  \n",
            "2             SBAModel | decouples | ServiceConsumer  \n",
            "3  ServiceConsumer | isDecoupledFrom | ServicePro...  \n",
            "4  SBAModel | achievesDecouplingVia | NetworkRepo...  \n",
            "Step4a pairs (raw→normalized): (1454, 2)\n",
            "            raw_relation    normalized_relation\n",
            "0         isIntroducedIn         isintroducedin\n",
            "1                enables                enables\n",
            "2              decouples              decouples\n",
            "3        isDecoupledFrom        isdecoupledfrom\n",
            "4  achievesDecouplingVia  achievesdecouplingvia\n",
            "Taxonomy nodes: (1895, 4)\n",
            "  node_id parent_id                       label  \\\n",
            "0  n00001    n01058  10MHzFrequencyDistribution   \n",
            "1  n00002    n01358   11ActiveStandbyRedundancy   \n",
            "2  n00003    n01594              3GPPCompliance   \n",
            "3  n00004    n01594               3GPPRelease14   \n",
            "4  n00005    n01594               3GPPRelease15   \n",
            "5  n00006    n00340    3GPPTS28312IntentSchemas   \n",
            "6  n00007    n01594                 3GPPTS29244   \n",
            "7  n00008    n00228                     464XLAT   \n",
            "\n",
            "                                                 uri  \n",
            "0  http://example.org/telecom#10MHzFrequencyDistr...  \n",
            "1  http://example.org/telecom#11ActiveStandbyRedu...  \n",
            "2          http://example.org/telecom#3GPPCompliance  \n",
            "3           http://example.org/telecom#3GPPRelease14  \n",
            "4           http://example.org/telecom#3GPPRelease15  \n",
            "5  http://example.org/telecom#3GPPTS28312IntentSc...  \n",
            "6             http://example.org/telecom#3GPPTS29244  \n",
            "7                 http://example.org/telecom#464XLAT  \n",
            "Class mappings (raw→node_id): (1895, 2)\n",
            "                    raw_class taxonomy_node\n",
            "0            TrafficIsolation        n01725\n",
            "1            PerformanceNeeds        n01199\n",
            "2  SliceSpecificSLAValidation        n01574\n",
            "3          PrometheusScraping        n01259\n",
            "4            DualConnectivity        n00398\n",
            "5                    Upgrades        n01824\n",
            "6     AutomatingObservability        n00116\n",
            "7         StatefulNFInstances        n01609\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from rapidfuzz import process, fuzz\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Load raw CSVs ----------\n",
        "df_dec = pd.read_csv(CSV_DECOMPOSE)\n",
        "df_tri = pd.read_csv(CSV_TRIPLES)\n",
        "df_rel = pd.read_csv(CSV_RELTYPE)\n",
        "df_cls = pd.read_csv(CSV_CLASSES)\n",
        "\n",
        "print(\"decompose.csv columns:\", df_dec.columns.tolist(), \"shape:\", df_dec.shape)\n",
        "print(\"triples.csv   columns:\", df_tri.columns.tolist(), \"shape:\", df_tri.shape)\n",
        "print(\"reltype.csv   columns:\", df_rel.columns.tolist(), \"shape:\", df_rel.shape)\n",
        "print(\"classes.csv   columns:\", df_cls.columns.tolist(), \"shape:\", df_cls.shape)\n",
        "\n",
        "# ---------- Step 2 dataset: explode Summary → Factoids ----------\n",
        "def parse_factoids_cell(cell: str):\n",
        "    if pd.isna(cell):\n",
        "        return []\n",
        "    txt = str(cell).strip().replace(\"\\r\", \"\\n\")\n",
        "    prelim = []\n",
        "    for line in txt.split(\"\\n\"):\n",
        "        # Allow comma-separated in same line\n",
        "        parts = [p for p in line.split(\",\") if p is not None]\n",
        "        for piece in parts:\n",
        "            t = piece.strip().strip('\"').strip(\"'\").strip()\n",
        "            if t:\n",
        "                prelim.append(t)\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for t in prelim:\n",
        "        t2 = t.strip().strip(\".;:\").strip()\n",
        "        # drop very short noise\n",
        "        if len(t2) < 3:\n",
        "            continue\n",
        "        if t2 not in seen:\n",
        "            seen.add(t2)\n",
        "            out.append(t2)\n",
        "    return out\n",
        "\n",
        "dec_rows = []\n",
        "sum_col = \"Summary sentence\"\n",
        "fac_col = \"Factoids\"\n",
        "for _, row in df_dec.iterrows():\n",
        "    complex_sent = str(row.get(sum_col, \"\")).strip()\n",
        "    facts = parse_factoids_cell(row.get(fac_col, \"\"))\n",
        "    for f in facts:\n",
        "        dec_rows.append({\"complex\": complex_sent, \"simple\": f})\n",
        "\n",
        "df_s2 = pd.DataFrame(dec_rows)\n",
        "print(\"Step2 pairs (complex→simple):\", df_s2.shape)\n",
        "print(df_s2.head(5))\n",
        "\n",
        "# ---------- Step 3 dataset: simple → triple_text ----------\n",
        "# Use 'Factoid' as the simple input. Prefer prebuilt 'Triplet' if present and in A|R|B format.\n",
        "def normalize_triplet_text(row):\n",
        "    trip = str(row.get(\"Triplet\", \"\")).strip()\n",
        "    if trip and \"|\" in trip:\n",
        "        # assume already formatted\n",
        "        return \" | \".join([p.strip() for p in trip.split(\"|\")[:3]])\n",
        "    # else, build from columns\n",
        "    a = str(row.get(\"Object 1\", \"\")).strip()\n",
        "    r = str(row.get(\"Relationship\", \"\")).strip()\n",
        "    b = str(row.get(\"Object 2\", \"\")).strip()\n",
        "    return f\"{a} | {r} | {b}\"\n",
        "\n",
        "df_s3 = pd.DataFrame({\n",
        "    \"simple\": df_tri[\"Factoid\"].astype(str).str.strip(),\n",
        "    \"triple_text\": df_tri.apply(normalize_triplet_text, axis=1).astype(str)\n",
        "})\n",
        "print(\"Step3 pairs (simple→triple_text):\", df_s3.shape)\n",
        "print(df_s3.head(5))\n",
        "\n",
        "# ---------- Step 4a dataset: raw_relation → normalized_relation ----------\n",
        "canon_relations = df_rel[\"relationship\"].dropna().astype(str).str.strip().unique().tolist()\n",
        "\n",
        "def best_match(label, choices, score_cut=70):\n",
        "    if label is None or str(label).strip()==\"\" or not choices:\n",
        "        return None, 0\n",
        "    label = str(label).strip()\n",
        "    match, score, _ = process.extractOne(label, choices, scorer=fuzz.token_sort_ratio)\n",
        "    if score >= score_cut:\n",
        "        return match, score\n",
        "    return None, score\n",
        "\n",
        "rel_pairs = []\n",
        "for raw in df_tri[\"Relationship\"].fillna(\"\").astype(str).tolist():\n",
        "    norm, score = best_match(raw, canon_relations, score_cut=70)\n",
        "    if norm is None:\n",
        "        if raw in canon_relations:\n",
        "            norm = raw\n",
        "        else:\n",
        "            norm = raw   # leave as-is if no good match\n",
        "    rel_pairs.append({\"raw_relation\": raw, \"normalized_relation\": norm})\n",
        "\n",
        "df_r = pd.DataFrame(rel_pairs).dropna()\n",
        "print(\"Step4a pairs (raw→normalized):\", df_r.shape)\n",
        "print(df_r.head(5))\n",
        "\n",
        "# ---------- Step 4b dataset & taxonomy from Class / Parent Class ----------\n",
        "if not {\"Class\",\"Parent Class\"}.issubset(df_cls.columns):\n",
        "    raise ValueError(\"classes.csv must contain 'Class' and 'Parent Class' columns.\")\n",
        "\n",
        "def sanitize(s: str):\n",
        "    s = str(s).strip()\n",
        "    return \"\".join(ch if ch.isalnum() else \"_\" for ch in s).strip(\"_\")\n",
        "\n",
        "# Build node table & parent links\n",
        "labels = set(df_cls[\"Class\"].dropna().astype(str).tolist()) | set(df_cls[\"Parent Class\"].dropna().astype(str).tolist())\n",
        "labels = {l for l in labels if l and l.lower() != \"nan\"}\n",
        "label2id = {lab: f\"n{idx+1:05d}\" for idx, lab in enumerate(sorted(labels))}\n",
        "\n",
        "taxonomy_rows = []\n",
        "for lab, nid in label2id.items():\n",
        "    taxonomy_rows.append({\"node_id\": nid, \"parent_id\": None, \"label\": lab, \"uri\": f\"{BASE_NS}{sanitize(lab)}\"})\n",
        "\n",
        "# Parent edges (we'll keep them in CSV via parent_id column for the child)\n",
        "parent_map = dict(zip(df_cls[\"Class\"].astype(str), df_cls[\"Parent Class\"].astype(str)))\n",
        "# Update parent_id for those with known parent\n",
        "for row in taxonomy_rows:\n",
        "    lab = row[\"label\"]\n",
        "    parent_label = parent_map.get(lab, None)\n",
        "    if parent_label in label2id:\n",
        "        row[\"parent_id\"] = label2id[parent_label]\n",
        "\n",
        "df_tax = pd.DataFrame(taxonomy_rows)\n",
        "df_tax.to_csv(TAXONOMY_CSV, index=False)\n",
        "\n",
        "# Class mappings (raw_class → its node_id)\n",
        "df_c = pd.DataFrame({\"raw_class\": [lab for lab in labels], \"taxonomy_node\": [label2id[lab] for lab in labels]})\n",
        "print(\"Taxonomy nodes:\", df_tax.shape)\n",
        "print(df_tax.head(8))\n",
        "print(\"Class mappings (raw→node_id):\", df_c.shape)\n",
        "print(df_c.head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce3349b",
      "metadata": {
        "id": "5ce3349b"
      },
      "source": [
        "## 3) Build Datasets for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "1d35bcfc",
      "metadata": {
        "id": "1d35bcfc",
        "outputId": "1ef69f45-6ecc-4ac9-97a6-6b6782dded2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6379e312e584c54962251381e63fd35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c262a55f18d44585bfc9cee09422a741",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1895 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets ready: \n",
            " - Step2 seq2seq: 2349 \n",
            " - Step3 seq2seq: 1454 \n",
            " - Step4a cls  : 1454 \n",
            " - Step4b cls  : 1895\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def build_seq2seq_dataset(df, source_col, target_col):\n",
        "    df = df[[source_col, target_col]].dropna().reset_index(drop=True)\n",
        "    return Dataset.from_pandas(df.rename(columns={source_col: \"input_text\", target_col: \"target_text\"}))\n",
        "\n",
        "def build_classification_dataset(df, text_col, label_col):\n",
        "    df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
        "    labels = sorted(df[label_col].astype(str).unique())\n",
        "    label2id = {lab:i for i, lab in enumerate(labels)}\n",
        "    id2label = {i:lab for lab, i in label2id.items()}\n",
        "    ds = Dataset.from_pandas(df.rename(columns={text_col:\"text\", label_col:\"label\"}))\n",
        "    ds = ds.map(lambda x: {\"label_id\": label2id[str(x[\"label\"])]})\n",
        "    return ds, label2id, id2label\n",
        "\n",
        "ds_s2 = build_seq2seq_dataset(df_s2, \"complex\", \"simple\")\n",
        "ds_s3 = build_seq2seq_dataset(df_s3, \"simple\", \"triple_text\")\n",
        "ds_rel, rel_label2id, rel_id2label = build_classification_dataset(df_r, \"raw_relation\", \"normalized_relation\")\n",
        "ds_cls, cls_label2id, cls_id2label = build_classification_dataset(df_c, \"raw_class\", \"taxonomy_node\")\n",
        "\n",
        "print(\"Datasets ready:\",\n",
        "      \"\\n - Step2 seq2seq:\", len(ds_s2),\n",
        "      \"\\n - Step3 seq2seq:\", len(ds_s3),\n",
        "      \"\\n - Step4a cls  :\", len(ds_rel),\n",
        "      \"\\n - Step4b cls  :\", len(ds_cls))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b6b6c2",
      "metadata": {
        "id": "f1b6b6c2"
      },
      "source": [
        "## 4) Train Step 2 — Complex → Simple (T5-small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a17c1adb",
      "metadata": {
        "id": "a17c1adb",
        "outputId": "d43f602f-ba3e-4a4f-eea3-e919fc5fa87f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc5d8660ffb94cc7ac350229c65a08aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2349 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4287081729.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_s2 = Trainer(model=model_s2, args=args_s2, train_dataset=tok_s2, data_collator=collator, tokenizer=tokenizer_t5)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='882' max='882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [882/882 01:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.543100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.098800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.050500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.996300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.832300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.744200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.732000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.783500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.759500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.774700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.706300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.588300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.652600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.619200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.692100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared Step 2 trainer (complex→simple). Un-comment trainer_s2.train() to run.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "\n",
        "tokenizer_t5 = T5TokenizerFast.from_pretrained(\"t5-small\")\n",
        "model_s2 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess_t5(batch, tokenizer, max_in=512, max_out=128):\n",
        "    model_inputs = tokenizer(batch[\"input_text\"], max_length=max_in, truncation=True)\n",
        "    labels = tokenizer(text_target=batch[\"target_text\"], max_length=max_out, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tok_s2 = ds_s2.map(lambda x: preprocess_t5(x, tokenizer_t5), batched=True, remove_columns=ds_s2.column_names)\n",
        "\n",
        "args_s2 = TrainingArguments(\n",
        "    output_dir=str(MODEL_DIR_SIMPLIFIER),\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50\n",
        ")\n",
        "# NOTE: We intentionally removed `evaluation_strategy` and `save_strategy` for older Transformers.\n",
        "collator = DataCollatorForSeq2Seq(tokenizer_t5, model=model_s2)\n",
        "trainer_s2 = Trainer(model=model_s2, args=args_s2, train_dataset=tok_s2, data_collator=collator, tokenizer=tokenizer_t5)\n",
        "\n",
        "# trainer_s2.train()  # ← un-comment to train\n",
        "# trainer_s2.save_model(MODEL_DIR_SIMPLIFIER)\n",
        "# tokenizer_t5.save_pretrained(MODEL_DIR_SIMPLIFIER)\n",
        "print(\"Prepared Step 2 trainer (complex→simple). Un-comment trainer_s2.train() to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb0b492",
      "metadata": {
        "id": "6bb0b492"
      },
      "source": [
        "## 5) Train Step 3 — Simple → Triple (T5-small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "bdd4e1a6",
      "metadata": {
        "id": "bdd4e1a6",
        "outputId": "31be3488-7019-4d53-dfab-46e81e6434e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b034e54f8bb94e3f85ce7ba3fcb032c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-844471508.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_s3 = Trainer(model=model_s3, args=args_s3, train_dataset=tok_s3, data_collator=collator, tokenizer=tokenizer_t5)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [546/546 00:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.470200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.268800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.960700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.852700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.682300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.705400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.493600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.527100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared Step 3 trainer (simple→triple). Un-comment trainer_s3.train() to run.\n"
          ]
        }
      ],
      "source": [
        "# Reuse same tokenizer for simplicity\n",
        "model_s3 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "tok_s3 = ds_s3.map(lambda x: preprocess_t5(x, tokenizer_t5), batched=True, remove_columns=ds_s3.column_names)\n",
        "\n",
        "args_s3 = TrainingArguments(\n",
        "    output_dir=str(MODEL_DIR_TRIPLE),\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50\n",
        ")\n",
        "# NOTE: We intentionally removed `evaluation_strategy` and `save_strategy` for older Transformers.\n",
        "trainer_s3 = Trainer(model=model_s3, args=args_s3, train_dataset=tok_s3, data_collator=collator, tokenizer=tokenizer_t5)\n",
        "\n",
        "# trainer_s3.train()  # ← un-comment to train\n",
        "# trainer_s3.save_model(MODEL_DIR_TRIPLE)\n",
        "# tokenizer_t5.save_pretrained(MODEL_DIR_TRIPLE)\n",
        "print(\"Prepared Step 3 trainer (simple→triple). Un-comment trainer_s3.train() to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a6d0eb",
      "metadata": {
        "id": "f8a6d0eb"
      },
      "source": [
        "## 6) Train Step 4a — Relation Normalizer (DistilBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "03aa6cda",
      "metadata": {
        "id": "03aa6cda",
        "outputId": "06498f10-36bc-448b-b911-f5c286c6f02d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f601cee781b44d5a27ee5b0173366f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3215930733.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_rel = Trainer(model=model_rel, args=args_rel, train_dataset=tok_ds_rel, data_collator=collator_rel, tokenizer=tok_rel)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='728' max='728' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [728/728 00:19, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.968900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.294100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.166800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.916200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.622300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.456100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.283100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.054700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.048600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.886600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.840800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.758700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.773000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared Step 4a trainer (relation normalization). Un-comment trainer_rel.train() to run.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import joblib\n",
        "\n",
        "tok_rel = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model_rel = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=len(rel_label2id),\n",
        "    id2label={i:l for i,l in enumerate(sorted(rel_label2id, key=lambda k: rel_label2id[k]))},\n",
        "    label2id=rel_label2id\n",
        ")\n",
        "\n",
        "def tokenize_cls(batch, tokenizer, max_len=64):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=max_len)\n",
        "\n",
        "tok_ds_rel = ds_rel.map(lambda x: tokenize_cls(x, tok_rel), batched=True)\n",
        "# Remove any leftover text/label columns and rename label_id→labels\n",
        "drop_cols = [c for c in tok_ds_rel.column_names if c in (\"text\",\"label\")]\n",
        "tok_ds_rel = tok_ds_rel.remove_columns(drop_cols)\n",
        "tok_ds_rel = tok_ds_rel.rename_column(\"label_id\", \"labels\")\n",
        "\n",
        "args_rel = TrainingArguments(\n",
        "    output_dir=str(MODEL_DIR_REL_CLASSIF),\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50\n",
        ")\n",
        "collator_rel = DataCollatorWithPadding(tokenizer=tok_rel)\n",
        "trainer_rel = Trainer(model=model_rel, args=args_rel, train_dataset=tok_ds_rel, data_collator=collator_rel, tokenizer=tok_rel)\n",
        "\n",
        "# trainer_rel.train()\n",
        "# trainer_rel.save_model(MODEL_DIR_REL_CLASSIF)\n",
        "# tok_rel.save_pretrained(MODEL_DIR_REL_CLASSIF)\n",
        "# joblib.dump(rel_label2id, LABELMAP_DIR / \"relation_label2id.joblib\")\n",
        "# joblib.dump({v:k for k,v in rel_label2id.items()}, LABELMAP_DIR / \"relation_id2label.joblib\")\n",
        "print(\"Prepared Step 4a trainer (relation normalization). Un-comment trainer_rel.train() to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27c8a65",
      "metadata": {
        "id": "e27c8a65"
      },
      "source": [
        "## 7) Train Step 4b — Class Taxonomy Mapper (DistilBERT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FVk5UN8JNtz8",
      "metadata": {
        "id": "FVk5UN8JNtz8"
      },
      "source": [
        "Quick summary: when (and why) use BERT here\n",
        "\n",
        "**When BERT is a good fit**\n",
        "\n",
        "We can reduce label space (e.g., per vertical / per parent class) and have dozens+ examples per label.\n",
        "\n",
        "We implement hierarchical classification (predict parent → child), metric learning (bi-encoder + nearest label), or retrieval-augmented matching (encode labels + pick nearest).\n",
        "\n",
        "We need low latency, fixed cost, or offline/on-prem inference.\n",
        "\n",
        "**Why BERT struggled in our case**\n",
        "\n",
        "We have ~1.8k classes and extremely sparse per-class data. With a flat softmax, the random baseline loss ≈ ln(1894) ≈ 7.54, so the model hovers ~7+ without lots of data or architectural tricks.\n",
        "\n",
        "**Practical hybrid recipe**\n",
        "\n",
        "Keep BERT for relations (4a) and/or for candidate retrieval (bi-encoder) → then let the OpenAI model do the final pick among 20–50 candidates (what v7 does). This keeps costs low and accuracy high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usr7PwrANiJr",
      "metadata": {
        "id": "usr7PwrANiJr"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY=\"example\"\n",
        "# optional: pick a model\n",
        "OPENAI_CLASS_MODEL=\"gpt-5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5a814d8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a814d8a",
        "outputId": "b72e5164-6ff3-40f1-9639-023bcb9afd1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings not found. Building taxonomy embeddings now (Section 7 fallback)...\n",
            "Saved: drive/MyDrive/MGR/trained_models/openai_class_mapper_4b/taxonomy_embeddings.npy and drive/MyDrive/MGR/trained_models/openai_class_mapper_4b/taxonomy_meta.json\n",
            "Wrote training_openai4b_mapping_preview.csv with 1831 rows\n",
            "Suggested thresholds: {'mid': 0.8028323650360107, 'high': 0.8661597073078156}\n"
          ]
        }
      ],
      "source": [
        "# Uses (or builds) taxonomy embeddings to preview mappings for objects from triples.csv,\n",
        "# computes simple score stats, suggests thresholds, and writes a debug CSV.\n",
        "# If embeddings are missing, this cell will build them here (no need to run Section 5 first).\n",
        "# Prereq: set OPENAI_API_KEY in your environment.\n",
        "\n",
        "# !pip install -U openai  # uncomment if the SDK isn't available\n",
        "\n",
        "import os, json, numpy as np, pandas as pd, re\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"OpenAI SDK not installed. Run `pip install -U openai` and re-run this cell.\") from e\n",
        "\n",
        "OPENAI_4B_DIR = Path(\"./drive/MyDrive/MGR/trained_models/openai_class_mapper_4b\")\n",
        "OPENAI_4B_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TAXO_EMB_NPY  = OPENAI_4B_DIR / \"taxonomy_embeddings.npy\"\n",
        "TAXO_META_JSON= OPENAI_4B_DIR / \"taxonomy_meta.json\"\n",
        "TAXONOMY_CSV  = Path(\"./data/taxonomy.csv\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "client = OpenAI()\n",
        "emb_model = \"text-embedding-3-large\"\n",
        "\n",
        "# --- Helpers ---\n",
        "def l2norm(x): return x / (np.linalg.norm(x, axis=1, keepdims=True) + 1e-9)\n",
        "def cosine_sim(a,b): return l2norm(a) @ l2norm(b).T\n",
        "\n",
        "def batched(iterable, n=256):\n",
        "    for i in range(0, len(iterable), n):\n",
        "        yield iterable[i:i+n]\n",
        "\n",
        "def embed_texts(texts, batch=256):\n",
        "    out = []\n",
        "    for chunk in batched(texts, batch):\n",
        "        resp = client.embeddings.create(model=emb_model, input=chunk)\n",
        "        out.extend([e.embedding for e in resp.data])\n",
        "    return np.array(out, dtype=\"float32\")\n",
        "\n",
        "def boost_with_path(object_str, node_text):\n",
        "    s = object_str.lower()\n",
        "    t = node_text.lower()\n",
        "    score = 0.0\n",
        "    leaf = node_text.split(\"⟂\")[0].strip().lower() if \"⟂\" in node_text else node_text.lower()\n",
        "    if s == leaf:\n",
        "        score += 0.06\n",
        "    toks = [w for w in re.findall(r\"[a-zA-Z0-9]+\", s) if len(w) > 2]\n",
        "    overlap = sum(1 for w in toks if w in t)\n",
        "    if overlap:\n",
        "        score += min(0.04, 0.01 * overlap)\n",
        "    return score\n",
        "\n",
        "# --- Ensure taxonomy is available ---\n",
        "if 'df_tax' not in globals():\n",
        "    assert TAXONOMY_CSV.exists(), \"Missing ./data/taxonomy.csv — run earlier sections to build taxonomy first.\"\n",
        "    df_tax = pd.read_csv(TAXONOMY_CSV)\n",
        "\n",
        "label_by_id = {row[\"node_id\"]: row[\"label\"] for _, row in df_tax.iterrows()}\n",
        "uri_by_id   = {row[\"node_id\"]: row[\"uri\"] for _, row in df_tax.iterrows()}\n",
        "parents     = {row[\"node_id\"]: row[\"parent_id\"] for _, row in df_tax.iterrows()}\n",
        "\n",
        "def label_with_context(node_id):\n",
        "    chain = []\n",
        "    cur = node_id\n",
        "    while cur:\n",
        "        chain.append(label_by_id[cur])\n",
        "        cur = parents.get(cur, None)\n",
        "    return \" ⟂ \".join(chain)\n",
        "\n",
        "# --- Build embeddings here if missing ---\n",
        "if not (TAXO_EMB_NPY.exists() and TAXO_META_JSON.exists()):\n",
        "    print(\"Embeddings not found. Building taxonomy embeddings now (Section 7 fallback)...\")\n",
        "    node_ids = sorted(label_by_id.keys())\n",
        "    texts = [label_with_context(nid) for nid in node_ids]\n",
        "    mat = embed_texts(texts, batch=256)\n",
        "    np.save(TAXO_EMB_NPY, mat)\n",
        "    with open(TAXO_META_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"model\": emb_model, \"node_ids\": node_ids, \"texts\": texts}, f, ensure_ascii=False, indent=2)\n",
        "    print(\"Saved:\", TAXO_EMB_NPY, \"and\", TAXO_META_JSON)\n",
        "\n",
        "# --- Load embeddings ---\n",
        "mat = np.load(TAXO_EMB_NPY)\n",
        "with open(TAXO_META_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "node_ids = meta[\"node_ids\"]\n",
        "texts    = meta[\"texts\"]\n",
        "assert mat.shape[0] == len(node_ids), \"Embedding matrix vs node_ids mismatch\"\n",
        "\n",
        "# --- Preview mappings from triples.csv ---\n",
        "if 'df_tri' not in globals():\n",
        "    df_tri = pd.read_csv(Path('./data/triples.csv'))\n",
        "\n",
        "objects = pd.Series(df_tri[\"Object 1\"].astype(str).tolist() + df_tri[\"Object 2\"].astype(str).tolist()) \\\n",
        "            .dropna().unique().tolist()\n",
        "\n",
        "obj_vecs = embed_texts(objects, batch=256)\n",
        "sims = cosine_sim(obj_vecs, mat)\n",
        "top_idx = sims.argsort(axis=1)[:, ::-1][:, :5]\n",
        "top_scores = np.take_along_axis(sims, top_idx, axis=1)\n",
        "\n",
        "# Apply small path-based boost and pick best\n",
        "best_ids = []\n",
        "best_scores = []\n",
        "for i, obj in enumerate(objects):\n",
        "    cand_idxs = top_idx[i]\n",
        "    cand_scores = top_scores[i]\n",
        "    adj = [cand_scores[j] + boost_with_path(obj, texts[cand_idxs[j]]) for j in range(len(cand_idxs))]\n",
        "    j = int(np.argmax(adj))\n",
        "    best_ids.append(node_ids[cand_idxs[j]])\n",
        "    best_scores.append(float(adj[j]))\n",
        "\n",
        "df_dbg = pd.DataFrame({\"object_text\": objects, \"node_id\": best_ids, \"score\": best_scores})\n",
        "df_dbg[\"label\"] = df_dbg[\"node_id\"].map(lambda nid: label_by_id.get(nid, nid))\n",
        "df_dbg[\"uri\"]   = df_dbg[\"node_id\"].map(lambda nid: uri_by_id.get(nid, nid))\n",
        "\n",
        "# Suggest thresholds from distribution\n",
        "p50, p75, p90 = np.percentile(df_dbg[\"score\"], [50,75,90])\n",
        "suggested = {\"mid\": float(max(0.70, p50)), \"high\": float(max(0.78, p75))}\n",
        "\n",
        "df_dbg.to_csv(\"training_openai4b_mapping_preview.csv\", index=False)\n",
        "with open(\"training_openai4b_thresholds.json\", \"w\") as f:\n",
        "    json.dump({\"p50\": float(p50), \"p75\": float(p75), \"p90\": float(p90), \"suggested\": suggested}, f, indent=2)\n",
        "\n",
        "print(\"Wrote training_openai4b_mapping_preview.csv with\", len(df_dbg), \"rows\")\n",
        "print(\"Suggested thresholds:\", suggested)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c7882c",
      "metadata": {
        "id": "15c7882c"
      },
      "source": [
        "## 8) (Optional) Simple Evaluation Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a19892",
      "metadata": {
        "id": "e1a19892"
      },
      "outputs": [],
      "source": [
        "# Add your own dev/test splits; quick accuracy eval templates:\n",
        "# import evaluate\n",
        "# metric = evaluate.load(\"accuracy\")\n",
        "# def compute_metrics(eval_pred):\n",
        "#     logits, labels = eval_pred\n",
        "#     preds = logits.argmax(-1)\n",
        "#     return metric.compute(predictions=preds, references=labels)\n",
        "print(\"See commented templates above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "PBsY8C9YjsBR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBsY8C9YjsBR",
        "outputId": "11abfffd-e8b1-4640-8b0a-385f5b8f90ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2297f1bd",
      "metadata": {
        "id": "2297f1bd"
      },
      "source": [
        "## 9) Save Artifacts\n",
        "\n",
        "Un-comment the save lines in Sections 4–7 after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ZdpSxIiLbWJl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZdpSxIiLbWJl",
        "outputId": "ad686604-c133-4ce9-f3be-6e21c931d8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: trained_models/ (stored 0%)\n",
            "  adding: trained_models/openai_class_mapper_4b/ (stored 0%)\n",
            "  adding: trained_models/openai_class_mapper_4b/taxonomy_meta.json (deflated 86%)\n",
            "  adding: trained_models/openai_class_mapper_4b/taxonomy_embeddings.npy (deflated 19%)\n",
            "  adding: trained_models/t5_simplifier_step2/ (stored 0%)\n",
            "  adding: trained_models/t5_simplifier_step2/tokenizer.json (deflated 74%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/ (stored 0%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/tokenizer.json (deflated 74%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/model.safetensors (deflated 9%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/generation_config.json (deflated 27%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/optimizer.pt (deflated 7%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/trainer_state.json (deflated 69%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/config.json (deflated 63%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/spiece.model (deflated 48%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/scheduler.pt (deflated 61%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/special_tokens_map.json (deflated 85%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/tokenizer_config.json (deflated 95%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-500/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/t5_simplifier_step2/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/t5_simplifier_step2/generation_config.json (deflated 27%)\n",
            "  adding: trained_models/t5_simplifier_step2/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/t5_simplifier_step2/config.json (deflated 63%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/ (stored 0%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/tokenizer.json (deflated 74%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/generation_config.json (deflated 27%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/optimizer.pt (deflated 7%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/trainer_state.json (deflated 73%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/config.json (deflated 63%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/spiece.model (deflated 48%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/scheduler.pt (deflated 62%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/special_tokens_map.json (deflated 85%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/tokenizer_config.json (deflated 95%)\n",
            "  adding: trained_models/t5_simplifier_step2/checkpoint-882/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/t5_simplifier_step2/spiece.model (deflated 48%)\n",
            "  adding: trained_models/t5_simplifier_step2/special_tokens_map.json (deflated 85%)\n",
            "  adding: trained_models/t5_simplifier_step2/tokenizer_config.json (deflated 95%)\n",
            "  adding: trained_models/t5_simplifier_step2/runs/ (stored 0%)\n",
            "  adding: trained_models/t5_simplifier_step2/runs/Sep04_23-26-07_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/t5_simplifier_step2/runs/Sep04_23-26-07_10ca3feee0c5/events.out.tfevents.1757028367.10ca3feee0c5.5714.0 (deflated 63%)\n",
            "  adding: trained_models/label_maps/ (stored 0%)\n",
            "  adding: trained_models/label_maps/relation_id2label.joblib (deflated 50%)\n",
            "  adding: trained_models/label_maps/class_label2id.joblib (deflated 73%)\n",
            "  adding: trained_models/label_maps/relation_label2id.joblib (deflated 50%)\n",
            "  adding: trained_models/label_maps/class_id2label.joblib (deflated 73%)\n",
            "  adding: trained_models/distilbert_relation_step4a/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/optimizer.pt (deflated 40%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/trainer_state.json (deflated 69%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/config.json (deflated 72%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/scheduler.pt (deflated 61%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-500/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_relation_step4a/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/optimizer.pt (deflated 40%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/trainer_state.json (deflated 72%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/config.json (deflated 72%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/scheduler.pt (deflated 62%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-728/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_relation_step4a/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/optimizer.pt (deflated 40%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/trainer_state.json (deflated 68%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/config.json (deflated 72%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/scheduler.pt (deflated 61%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-455/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_relation_step4a/config.json (deflated 72%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/optimizer.pt (deflated 40%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/trainer_state.json (deflated 73%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/config.json (deflated 72%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/scheduler.pt (deflated 61%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-910/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_relation_step4a/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_relation_step4a/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/optimizer.pt (deflated 40%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/trainer_state.json (deflated 64%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/config.json (deflated 72%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/scheduler.pt (deflated 62%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_relation_step4a/checkpoint-273/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-36-41_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-36-41_10ca3feee0c5/events.out.tfevents.1757029002.10ca3feee0c5.5714.3 (deflated 70%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-38-02_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-38-02_10ca3feee0c5/events.out.tfevents.1757029083.10ca3feee0c5.5714.6 (deflated 70%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-30-04_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-30-04_10ca3feee0c5/events.out.tfevents.1757028604.10ca3feee0c5.5714.2 (deflated 71%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-37-29_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-37-29_10ca3feee0c5/events.out.tfevents.1757029050.10ca3feee0c5.5714.5 (deflated 70%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-37-12_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/distilbert_relation_step4a/runs/Sep04_23-37-12_10ca3feee0c5/events.out.tfevents.1757029033.10ca3feee0c5.5714.4 (deflated 70%)\n",
            "  adding: trained_models/distilbert_class_step4b/ (stored 0%)\n",
            "  adding: trained_models/distilbert_class_step4b/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/ (stored 0%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/optimizer.pt (deflated 38%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/trainer_state.json (deflated 69%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/scheduler.pt (deflated 61%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-500/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_class_step4b/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_class_step4b/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/ (stored 0%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/optimizer.pt (deflated 38%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/trainer_state.json (deflated 69%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/scheduler.pt (deflated 61%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-490/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_class_step4b/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_class_step4b/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/ (stored 0%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/tokenizer.json (deflated 71%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/optimizer.pt (deflated 38%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/trainer_state.json (deflated 73%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/training_args.bin (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/vocab.txt (deflated 53%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/scheduler.pt (deflated 62%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/tokenizer_config.json (deflated 75%)\n",
            "  adding: trained_models/distilbert_class_step4b/checkpoint-952/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/distilbert_class_step4b/runs/ (stored 0%)\n",
            "  adding: trained_models/distilbert_class_step4b/runs/Sep04_23-44-52_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/distilbert_class_step4b/runs/Sep04_23-44-52_10ca3feee0c5/events.out.tfevents.1757029493.10ca3feee0c5.5714.8 (deflated 74%)\n",
            "  adding: trained_models/distilbert_class_step4b/runs/Sep04_23-44-52_10ca3feee0c5/events.out.tfevents.1757029505.10ca3feee0c5.5714.9 (deflated 30%)\n",
            "  adding: trained_models/distilbert_class_step4b/runs/Sep04_23-38-42_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/distilbert_class_step4b/runs/Sep04_23-38-42_10ca3feee0c5/events.out.tfevents.1757029123.10ca3feee0c5.5714.7 (deflated 74%)\n",
            "  adding: trained_models/t5_triple_step3/ (stored 0%)\n",
            "  adding: trained_models/t5_triple_step3/tokenizer.json (deflated 74%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/ (stored 0%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/tokenizer.json (deflated 74%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/generation_config.json (deflated 27%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/optimizer.pt (deflated 7%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/trainer_state.json (deflated 69%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/training_args.bin (deflated 54%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/config.json (deflated 63%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/spiece.model (deflated 48%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/scheduler.pt (deflated 62%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/special_tokens_map.json (deflated 85%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/tokenizer_config.json (deflated 95%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-546/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/ (stored 0%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/tokenizer.json (deflated 74%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/model.safetensors (deflated 9%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/generation_config.json (deflated 27%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/optimizer.pt (deflated 7%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/trainer_state.json (deflated 70%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/training_args.bin (deflated 54%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/config.json (deflated 63%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/spiece.model (deflated 48%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/scheduler.pt (deflated 61%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/special_tokens_map.json (deflated 85%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/tokenizer_config.json (deflated 95%)\n",
            "  adding: trained_models/t5_triple_step3/checkpoint-500/rng_state.pth (deflated 26%)\n",
            "  adding: trained_models/t5_triple_step3/model.safetensors (deflated 8%)\n",
            "  adding: trained_models/t5_triple_step3/generation_config.json (deflated 27%)\n",
            "  adding: trained_models/t5_triple_step3/training_args.bin (deflated 54%)\n",
            "  adding: trained_models/t5_triple_step3/config.json (deflated 63%)\n",
            "  adding: trained_models/t5_triple_step3/spiece.model (deflated 48%)\n",
            "  adding: trained_models/t5_triple_step3/special_tokens_map.json (deflated 85%)\n",
            "  adding: trained_models/t5_triple_step3/tokenizer_config.json (deflated 95%)\n",
            "  adding: trained_models/t5_triple_step3/runs/ (stored 0%)\n",
            "  adding: trained_models/t5_triple_step3/runs/Sep04_23-28-48_10ca3feee0c5/ (stored 0%)\n",
            "  adding: trained_models/t5_triple_step3/runs/Sep04_23-28-48_10ca3feee0c5/events.out.tfevents.1757028529.10ca3feee0c5.5714.1 (deflated 62%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f0da0b00-576f-408e-aeca-7bb414fe1c31\", \"trained_models.zip\", 8294182061)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Change this to the folder you want\n",
        "FOLDER = \"/content/trained_models\"\n",
        "ARCHIVE = \"/content/trained_models.zip\"\n",
        "\n",
        "# Zip it (recursive, store paths relative to the folder’s parent)\n",
        "!cd \"$(dirname \"$FOLDER\")\" && zip -r \"$(basename \"$ARCHIVE\")\" \"$(basename \"$FOLDER\")\"\n",
        "\n",
        "from google.colab import files\n",
        "files.download(ARCHIVE)  # opens a browser download dialog"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
