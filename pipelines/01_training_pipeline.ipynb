{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ff309e4",
      "metadata": {
        "id": "6ff309e4"
      },
      "source": [
        "# 01 — Training Pipeline (Adapted to Your CSVs)\n",
        "\n",
        "This notebook is **tailored to the retrieved files**:\n",
        "\n",
        "- **`./data/decompose.csv`** → Step 2 (complex → simple). We **explode** each row so that a *Summary sentence* maps to **multiple** simple *Factoids*.\n",
        "- **`./data/triples.csv`** → Step 3 (simple → triple). We build targets in the format **`Object 1 | Relationship | Object 2`**.\n",
        "- **`./data/reltype.csv`** → Step 4a (relation normalization). We create a training set of **raw relation strings** (from `triples.csv`) mapped to **canonical relation labels** (from `reltype.csv`) using **exact+fuzzy matching**.\n",
        "- **`./data/classes.csv`** → Step 4b (class taxonomy mapping). We build a **taxonomy** from the hierarchical columns and map each **Entity** to a **leaf node**. The notebook emits a `taxonomy.csv` that the ingestion notebook uses.\n",
        "\n",
        "We keep models small and finetune-ready:\n",
        "- **T5-small** for Steps 2 & 3 (seq2seq)\n",
        "- **DistilBERT** for Steps 4a & 4b (classification)\n",
        "\n",
        "> Training is commented out by default, so it should be uncommented to run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91aae7dd",
      "metadata": {
        "id": "91aae7dd"
      },
      "source": [
        "## 0) Environment Setup\n",
        "\n",
        "If needed, install packages. If you're offline here, make sure the environment already has them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45c89ceb",
      "metadata": {
        "id": "45c89ceb"
      },
      "outputs": [],
      "source": [
        "# !pip install -U pip\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "# !pip install transformers datasets accelerate sentencepiece scikit-learn evaluate\n",
        "# !pip install pandas numpy tqdm matplotlib python-levenshtein rapidfuzz\n",
        "# !pip install nltk spacy rdflib joblib\n",
        "# !python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "282619a6",
      "metadata": {
        "id": "282619a6"
      },
      "source": [
        "## 1) Config & Paths\n",
        "\n",
        "We point directly at the CSVs uploaded under `./data/`. Outputs go to `./trained_models/` within this working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df0d1ff",
      "metadata": {
        "id": "7df0d1ff"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "DATA_DIR = Path(\"./data\")\n",
        "\n",
        "CSV_DECOMPOSE = DATA_DIR / \"decompose.csv\"   # Source, Summary sentence, Factoids\n",
        "CSV_TRIPLES   = DATA_DIR / \"triples.csv\"     # Factoid, Triplet, Object 1, Relationship, Object 2\n",
        "CSV_RELTYPE   = DATA_DIR / \"reltype.csv\"     # relationship, type, domain, range, otherCharacteristics\n",
        "CSV_CLASSES   = DATA_DIR / \"classes.csv\"     # Class, Parent Class\n",
        "\n",
        "OUT_DIR = Path(\"./drive/MyDrive/MGR/trained_models\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_DIR_SIMPLIFIER   = OUT_DIR / \"t5_simplifier_step2\"\n",
        "MODEL_DIR_TRIPLE       = OUT_DIR / \"t5_triple_step3\"\n",
        "MODEL_DIR_REL_CLASSIF  = OUT_DIR / \"distilbert_relation_step4a\"\n",
        "MODEL_DIR_CLASS_CLASSIF= OUT_DIR / \"distilbert_class_step4b\"\n",
        "\n",
        "LABELMAP_DIR = OUT_DIR / \"label_maps\"\n",
        "LABELMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_OUT = Path(\"./data\")\n",
        "DATA_OUT.mkdir(parents=True, exist_ok=True)\n",
        "TAXONOMY_CSV = DATA_OUT / \"taxonomy.csv\"\n",
        "\n",
        "BASE_NS = \"http://example.org/telecom#\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45ed6f3",
      "metadata": {
        "id": "d45ed6f3"
      },
      "source": [
        "## 2) Inspect & Parse Your CSVs\n",
        "\n",
        "We auto-parse & normalize columns from four files:\n",
        "- `decompose.csv` → pairs: **(Summary sentence → individual factoid)**\n",
        "- `triples.csv` → pairs: **(Simple sentence → `Object 1 | Relationship | Object 2`)**\n",
        "- `reltype.csv` → canonical **relation vocabulary** (with `domain`, `range`, `type`)\n",
        "- `classes.csv` → hierarchical **class taxonomy** (we emit `taxonomy.csv` for later use)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fbb093",
      "metadata": {
        "id": "a8fbb093"
      },
      "outputs": [],
      "source": [
        "from rapidfuzz import process, fuzz\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Load raw CSVs ----------\n",
        "df_dec = pd.read_csv(CSV_DECOMPOSE)\n",
        "df_tri = pd.read_csv(CSV_TRIPLES)\n",
        "df_rel = pd.read_csv(CSV_RELTYPE)\n",
        "df_cls = pd.read_csv(CSV_CLASSES)\n",
        "\n",
        "print(\"decompose.csv columns:\", df_dec.columns.tolist(), \"shape:\", df_dec.shape)\n",
        "print(\"triples.csv   columns:\", df_tri.columns.tolist(), \"shape:\", df_tri.shape)\n",
        "print(\"reltype.csv   columns:\", df_rel.columns.tolist(), \"shape:\", df_rel.shape)\n",
        "print(\"classes.csv   columns:\", df_cls.columns.tolist(), \"shape:\", df_cls.shape)\n",
        "\n",
        "# ---------- Step 2 dataset: explode Summary → Factoids ----------\n",
        "def parse_factoids_cell(cell: str):\n",
        "    if pd.isna(cell):\n",
        "        return []\n",
        "    txt = str(cell).strip().replace(\"\\r\", \"\\n\")\n",
        "    prelim = []\n",
        "    for line in txt.split(\"\\n\"):\n",
        "        # Allow comma-separated in same line\n",
        "        parts = [p for p in line.split(\",\") if p is not None]\n",
        "        for piece in parts:\n",
        "            t = piece.strip().strip('\"').strip(\"'\").strip()\n",
        "            if t:\n",
        "                prelim.append(t)\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for t in prelim:\n",
        "        t2 = t.strip().strip(\".;:\").strip()\n",
        "        # drop very short noise\n",
        "        if len(t2) < 3:\n",
        "            continue\n",
        "        if t2 not in seen:\n",
        "            seen.add(t2)\n",
        "            out.append(t2)\n",
        "    return out\n",
        "\n",
        "dec_rows = []\n",
        "sum_col = \"Summary sentence\"\n",
        "fac_col = \"Factoids\"\n",
        "for _, row in df_dec.iterrows():\n",
        "    complex_sent = str(row.get(sum_col, \"\")).strip()\n",
        "    facts = parse_factoids_cell(row.get(fac_col, \"\"))\n",
        "    for f in facts:\n",
        "        dec_rows.append({\"complex\": complex_sent, \"simple\": f})\n",
        "\n",
        "df_s2 = pd.DataFrame(dec_rows)\n",
        "print(\"Step2 pairs (complex→simple):\", df_s2.shape)\n",
        "print(df_s2.head(5))\n",
        "\n",
        "# ---------- Step 3 dataset: simple → triple_text ----------\n",
        "# Use 'Factoid' as the simple input. Prefer prebuilt 'Triplet' if present and in A|R|B format.\n",
        "def normalize_triplet_text(row):\n",
        "    trip = str(row.get(\"Triplet\", \"\")).strip()\n",
        "    if trip and \"|\" in trip:\n",
        "        # assume already formatted\n",
        "        return \" | \".join([p.strip() for p in trip.split(\"|\")[:3]])\n",
        "    # else, build from columns\n",
        "    a = str(row.get(\"Object 1\", \"\")).strip()\n",
        "    r = str(row.get(\"Relationship\", \"\")).strip()\n",
        "    b = str(row.get(\"Object 2\", \"\")).strip()\n",
        "    return f\"{a} | {r} | {b}\"\n",
        "\n",
        "df_s3 = pd.DataFrame({\n",
        "    \"simple\": df_tri[\"Factoid\"].astype(str).str.strip(),\n",
        "    \"triple_text\": df_tri.apply(normalize_triplet_text, axis=1).astype(str)\n",
        "})\n",
        "print(\"Step3 pairs (simple→triple_text):\", df_s3.shape)\n",
        "print(df_s3.head(5))\n",
        "\n",
        "# ---------- Step 4a dataset: raw_relation → normalized_relation ----------\n",
        "canon_relations = df_rel[\"relationship\"].dropna().astype(str).str.strip().unique().tolist()\n",
        "\n",
        "def best_match(label, choices, score_cut=70):\n",
        "    if label is None or str(label).strip()==\"\" or not choices:\n",
        "        return None, 0\n",
        "    label = str(label).strip()\n",
        "    match, score, _ = process.extractOne(label, choices, scorer=fuzz.token_sort_ratio)\n",
        "    if score >= score_cut:\n",
        "        return match, score\n",
        "    return None, score\n",
        "\n",
        "rel_pairs = []\n",
        "for raw in df_tri[\"Relationship\"].fillna(\"\").astype(str).tolist():\n",
        "    norm, score = best_match(raw, canon_relations, score_cut=70)\n",
        "    if norm is None:\n",
        "        if raw in canon_relations:\n",
        "            norm = raw\n",
        "        else:\n",
        "            norm = raw   # leave as-is if no good match\n",
        "    rel_pairs.append({\"raw_relation\": raw, \"normalized_relation\": norm})\n",
        "\n",
        "df_r = pd.DataFrame(rel_pairs).dropna()\n",
        "print(\"Step4a pairs (raw→normalized):\", df_r.shape)\n",
        "print(df_r.head(5))\n",
        "\n",
        "# ---------- Step 4b dataset & taxonomy from Class / Parent Class ----------\n",
        "if not {\"Class\",\"Parent Class\"}.issubset(df_cls.columns):\n",
        "    raise ValueError(\"classes.csv must contain 'Class' and 'Parent Class' columns.\")\n",
        "\n",
        "def sanitize(s: str):\n",
        "    s = str(s).strip()\n",
        "    return \"\".join(ch if ch.isalnum() else \"_\" for ch in s).strip(\"_\")\n",
        "\n",
        "# Build node table & parent links\n",
        "labels = set(df_cls[\"Class\"].dropna().astype(str).tolist()) | set(df_cls[\"Parent Class\"].dropna().astype(str).tolist())\n",
        "labels = {l for l in labels if l and l.lower() != \"nan\"}\n",
        "label2id = {lab: f\"n{idx+1:05d}\" for idx, lab in enumerate(sorted(labels))}\n",
        "\n",
        "taxonomy_rows = []\n",
        "for lab, nid in label2id.items():\n",
        "    taxonomy_rows.append({\"node_id\": nid, \"parent_id\": None, \"label\": lab, \"uri\": f\"{BASE_NS}{sanitize(lab)}\"})\n",
        "\n",
        "# Parent edges (we'll keep them in CSV via parent_id column for the child)\n",
        "parent_map = dict(zip(df_cls[\"Class\"].astype(str), df_cls[\"Parent Class\"].astype(str)))\n",
        "# Update parent_id for those with known parent\n",
        "for row in taxonomy_rows:\n",
        "    lab = row[\"label\"]\n",
        "    parent_label = parent_map.get(lab, None)\n",
        "    if parent_label in label2id:\n",
        "        row[\"parent_id\"] = label2id[parent_label]\n",
        "\n",
        "df_tax = pd.DataFrame(taxonomy_rows)\n",
        "df_tax.to_csv(TAXONOMY_CSV, index=False)\n",
        "\n",
        "# Class mappings (raw_class → its node_id)\n",
        "df_c = pd.DataFrame({\"raw_class\": [lab for lab in labels], \"taxonomy_node\": [label2id[lab] for lab in labels]})\n",
        "print(\"Taxonomy nodes:\", df_tax.shape)\n",
        "print(df_tax.head(8))\n",
        "print(\"Class mappings (raw→node_id):\", df_c.shape)\n",
        "print(df_c.head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce3349b",
      "metadata": {
        "id": "5ce3349b"
      },
      "source": [
        "## 3) Build Datasets for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d35bcfc",
      "metadata": {
        "id": "1d35bcfc"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def build_seq2seq_dataset(df, source_col, target_col):\n",
        "    df = df[[source_col, target_col]].dropna().reset_index(drop=True)\n",
        "    return Dataset.from_pandas(df.rename(columns={source_col: \"input_text\", target_col: \"target_text\"}))\n",
        "\n",
        "def build_classification_dataset(df, text_col, label_col):\n",
        "    df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
        "    labels = sorted(df[label_col].astype(str).unique())\n",
        "    label2id = {lab:i for i, lab in enumerate(labels)}\n",
        "    id2label = {i:lab for lab, i in label2id.items()}\n",
        "    ds = Dataset.from_pandas(df.rename(columns={text_col:\"text\", label_col:\"label\"}))\n",
        "    ds = ds.map(lambda x: {\"label_id\": label2id[str(x[\"label\"])]})\n",
        "    return ds, label2id, id2label\n",
        "\n",
        "ds_s2 = build_seq2seq_dataset(df_s2, \"complex\", \"simple\")\n",
        "ds_s3 = build_seq2seq_dataset(df_s3, \"simple\", \"triple_text\")\n",
        "ds_rel, rel_label2id, rel_id2label = build_classification_dataset(df_r, \"raw_relation\", \"normalized_relation\")\n",
        "ds_cls, cls_label2id, cls_id2label = build_classification_dataset(df_c, \"raw_class\", \"taxonomy_node\")\n",
        "\n",
        "print(\"Datasets ready:\",\n",
        "      \"\\n - Step2 seq2seq:\", len(ds_s2),\n",
        "      \"\\n - Step3 seq2seq:\", len(ds_s3),\n",
        "      \"\\n - Step4a cls  :\", len(ds_rel),\n",
        "      \"\\n - Step4b cls  :\", len(ds_cls))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b6b6c2",
      "metadata": {
        "id": "f1b6b6c2"
      },
      "source": [
        "## 4) Train Step 2 — Complex → Simple (T5-small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17c1adb",
      "metadata": {
        "id": "a17c1adb"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "\n",
        "tokenizer_t5 = T5TokenizerFast.from_pretrained(\"t5-small\")\n",
        "model_s2 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess_t5(batch, tokenizer, max_in=512, max_out=128):\n",
        "    model_inputs = tokenizer(batch[\"input_text\"], max_length=max_in, truncation=True)\n",
        "    labels = tokenizer(text_target=batch[\"target_text\"], max_length=max_out, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tok_s2 = ds_s2.map(lambda x: preprocess_t5(x, tokenizer_t5), batched=True, remove_columns=ds_s2.column_names)\n",
        "\n",
        "args_s2 = TrainingArguments(\n",
        "    output_dir=str(MODEL_DIR_SIMPLIFIER),\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50\n",
        ")\n",
        "# NOTE: We intentionally removed `evaluation_strategy` and `save_strategy` for older Transformers.\n",
        "collator = DataCollatorForSeq2Seq(tokenizer_t5, model=model_s2)\n",
        "trainer_s2 = Trainer(model=model_s2, args=args_s2, train_dataset=tok_s2, data_collator=collator, tokenizer=tokenizer_t5)\n",
        "\n",
        "# trainer_s2.train()  # ← un-comment to train\n",
        "# trainer_s2.save_model(MODEL_DIR_SIMPLIFIER)\n",
        "# tokenizer_t5.save_pretrained(MODEL_DIR_SIMPLIFIER)\n",
        "print(\"Prepared Step 2 trainer (complex→simple). Un-comment trainer_s2.train() to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb0b492",
      "metadata": {
        "id": "6bb0b492"
      },
      "source": [
        "## 5) Train Step 3 — Simple → Triple (T5-small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd4e1a6",
      "metadata": {
        "id": "bdd4e1a6"
      },
      "outputs": [],
      "source": [
        "# Reuse same tokenizer for simplicity\n",
        "model_s3 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "tok_s3 = ds_s3.map(lambda x: preprocess_t5(x, tokenizer_t5), batched=True, remove_columns=ds_s3.column_names)\n",
        "\n",
        "args_s3 = TrainingArguments(\n",
        "    output_dir=str(MODEL_DIR_TRIPLE),\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50\n",
        ")\n",
        "# NOTE: We intentionally removed `evaluation_strategy` and `save_strategy` for older Transformers.\n",
        "trainer_s3 = Trainer(model=model_s3, args=args_s3, train_dataset=tok_s3, data_collator=collator, tokenizer=tokenizer_t5)\n",
        "\n",
        "# trainer_s3.train()  # ← un-comment to train\n",
        "# trainer_s3.save_model(MODEL_DIR_TRIPLE)\n",
        "# tokenizer_t5.save_pretrained(MODEL_DIR_TRIPLE)\n",
        "print(\"Prepared Step 3 trainer (simple→triple). Un-comment trainer_s3.train() to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a6d0eb",
      "metadata": {
        "id": "f8a6d0eb"
      },
      "source": [
        "## 6) Train Step 4a — Relation Normalizer (DistilBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03aa6cda",
      "metadata": {
        "id": "03aa6cda"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import joblib\n",
        "\n",
        "tok_rel = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model_rel = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=len(rel_label2id),\n",
        "    id2label={i:l for i,l in enumerate(sorted(rel_label2id, key=lambda k: rel_label2id[k]))},\n",
        "    label2id=rel_label2id\n",
        ")\n",
        "\n",
        "def tokenize_cls(batch, tokenizer, max_len=64):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=max_len)\n",
        "\n",
        "tok_ds_rel = ds_rel.map(lambda x: tokenize_cls(x, tok_rel), batched=True)\n",
        "# Remove any leftover text/label columns and rename label_id→labels\n",
        "drop_cols = [c for c in tok_ds_rel.column_names if c in (\"text\",\"label\")]\n",
        "tok_ds_rel = tok_ds_rel.remove_columns(drop_cols)\n",
        "tok_ds_rel = tok_ds_rel.rename_column(\"label_id\", \"labels\")\n",
        "\n",
        "args_rel = TrainingArguments(\n",
        "    output_dir=str(MODEL_DIR_REL_CLASSIF),\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50\n",
        ")\n",
        "collator_rel = DataCollatorWithPadding(tokenizer=tok_rel)\n",
        "trainer_rel = Trainer(model=model_rel, args=args_rel, train_dataset=tok_ds_rel, data_collator=collator_rel, tokenizer=tok_rel)\n",
        "\n",
        "# trainer_rel.train()\n",
        "# trainer_rel.save_model(MODEL_DIR_REL_CLASSIF)\n",
        "# tok_rel.save_pretrained(MODEL_DIR_REL_CLASSIF)\n",
        "# joblib.dump(rel_label2id, LABELMAP_DIR / \"relation_label2id.joblib\")\n",
        "# joblib.dump({v:k for k,v in rel_label2id.items()}, LABELMAP_DIR / \"relation_id2label.joblib\")\n",
        "print(\"Prepared Step 4a trainer (relation normalization). Un-comment trainer_rel.train() to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27c8a65",
      "metadata": {
        "id": "e27c8a65"
      },
      "source": [
        "## 7) Train Step 4b — Class Taxonomy Mapper (DistilBERT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick summary: when (and why) use BERT here\n",
        "\n",
        "**When BERT is a good fit**\n",
        "\n",
        "We can reduce label space (e.g., per vertical / per parent class) and have dozens+ examples per label.\n",
        "\n",
        "We implement hierarchical classification (predict parent → child), metric learning (bi-encoder + nearest label), or retrieval-augmented matching (encode labels + pick nearest).\n",
        "\n",
        "We need low latency, fixed cost, or offline/on-prem inference.\n",
        "\n",
        "**Why BERT struggled in our case**\n",
        "\n",
        "We have ~1.8k classes and extremely sparse per-class data. With a flat softmax, the random baseline loss ≈ ln(1894) ≈ 7.54, so the model hovers ~7+ without lots of data or architectural tricks.\n",
        "\n",
        "**Practical hybrid recipe**\n",
        "\n",
        "Keep BERT for relations (4a) and/or for candidate retrieval (bi-encoder) → then let the OpenAI model do the final pick among 20–50 candidates (what v7 does). This keeps costs low and accuracy high."
      ],
      "metadata": {
        "id": "FVk5UN8JNtz8"
      },
      "id": "FVk5UN8JNtz8"
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY=\"example\"\n",
        "# optional: pick a model\n",
        "OPENAI_CLASS_MODEL=\"gpt-5\""
      ],
      "metadata": {
        "id": "usr7PwrANiJr"
      },
      "id": "usr7PwrANiJr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a814d8a",
      "metadata": {
        "id": "5a814d8a"
      },
      "outputs": [],
      "source": [
        "# Uses (or builds) taxonomy embeddings to preview mappings for objects from triples.csv,\n",
        "# computes simple score stats, suggests thresholds, and writes a debug CSV.\n",
        "# If embeddings are missing, this cell will build them here (no need to run Section 5 first).\n",
        "# Prereq: set OPENAI_API_KEY in your environment.\n",
        "\n",
        "# !pip install -U openai  # uncomment if the SDK isn't available\n",
        "\n",
        "import os, json, numpy as np, pandas as pd, re\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"OpenAI SDK not installed. Run `pip install -U openai` and re-run this cell.\") from e\n",
        "\n",
        "OPENAI_4B_DIR = Path(\"./drive/MyDrive/MGR/trained_models/openai_class_mapper_4b\")\n",
        "OPENAI_4B_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TAXO_EMB_NPY  = OPENAI_4B_DIR / \"taxonomy_embeddings.npy\"\n",
        "TAXO_META_JSON= OPENAI_4B_DIR / \"taxonomy_meta.json\"\n",
        "TAXONOMY_CSV  = Path(\"./data/taxonomy.csv\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "client = OpenAI()\n",
        "emb_model = \"text-embedding-3-large\"\n",
        "\n",
        "# --- Helpers ---\n",
        "def l2norm(x): return x / (np.linalg.norm(x, axis=1, keepdims=True) + 1e-9)\n",
        "def cosine_sim(a,b): return l2norm(a) @ l2norm(b).T\n",
        "\n",
        "def batched(iterable, n=256):\n",
        "    for i in range(0, len(iterable), n):\n",
        "        yield iterable[i:i+n]\n",
        "\n",
        "def embed_texts(texts, batch=256):\n",
        "    out = []\n",
        "    for chunk in batched(texts, batch):\n",
        "        resp = client.embeddings.create(model=emb_model, input=chunk)\n",
        "        out.extend([e.embedding for e in resp.data])\n",
        "    return np.array(out, dtype=\"float32\")\n",
        "\n",
        "def boost_with_path(object_str, node_text):\n",
        "    s = object_str.lower()\n",
        "    t = node_text.lower()\n",
        "    score = 0.0\n",
        "    leaf = node_text.split(\"⟂\")[0].strip().lower() if \"⟂\" in node_text else node_text.lower()\n",
        "    if s == leaf:\n",
        "        score += 0.06\n",
        "    toks = [w for w in re.findall(r\"[a-zA-Z0-9]+\", s) if len(w) > 2]\n",
        "    overlap = sum(1 for w in toks if w in t)\n",
        "    if overlap:\n",
        "        score += min(0.04, 0.01 * overlap)\n",
        "    return score\n",
        "\n",
        "# --- Ensure taxonomy is available ---\n",
        "if 'df_tax' not in globals():\n",
        "    assert TAXONOMY_CSV.exists(), \"Missing ./data/taxonomy.csv — run earlier sections to build taxonomy first.\"\n",
        "    df_tax = pd.read_csv(TAXONOMY_CSV)\n",
        "\n",
        "label_by_id = {row[\"node_id\"]: row[\"label\"] for _, row in df_tax.iterrows()}\n",
        "uri_by_id   = {row[\"node_id\"]: row[\"uri\"] for _, row in df_tax.iterrows()}\n",
        "parents     = {row[\"node_id\"]: row[\"parent_id\"] for _, row in df_tax.iterrows()}\n",
        "\n",
        "def label_with_context(node_id):\n",
        "    chain = []\n",
        "    cur = node_id\n",
        "    while cur:\n",
        "        chain.append(label_by_id[cur])\n",
        "        cur = parents.get(cur, None)\n",
        "    return \" ⟂ \".join(chain)\n",
        "\n",
        "# --- Build embeddings here if missing ---\n",
        "if not (TAXO_EMB_NPY.exists() and TAXO_META_JSON.exists()):\n",
        "    print(\"Embeddings not found. Building taxonomy embeddings now (Section 7 fallback)...\")\n",
        "    node_ids = sorted(label_by_id.keys())\n",
        "    texts = [label_with_context(nid) for nid in node_ids]\n",
        "    mat = embed_texts(texts, batch=256)\n",
        "    np.save(TAXO_EMB_NPY, mat)\n",
        "    with open(TAXO_META_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"model\": emb_model, \"node_ids\": node_ids, \"texts\": texts}, f, ensure_ascii=False, indent=2)\n",
        "    print(\"Saved:\", TAXO_EMB_NPY, \"and\", TAXO_META_JSON)\n",
        "\n",
        "# --- Load embeddings ---\n",
        "mat = np.load(TAXO_EMB_NPY)\n",
        "with open(TAXO_META_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "node_ids = meta[\"node_ids\"]\n",
        "texts    = meta[\"texts\"]\n",
        "assert mat.shape[0] == len(node_ids), \"Embedding matrix vs node_ids mismatch\"\n",
        "\n",
        "# --- Preview mappings from triples.csv ---\n",
        "if 'df_tri' not in globals():\n",
        "    df_tri = pd.read_csv(Path('./data/triples.csv'))\n",
        "\n",
        "objects = pd.Series(df_tri[\"Object 1\"].astype(str).tolist() + df_tri[\"Object 2\"].astype(str).tolist()) \\\n",
        "            .dropna().unique().tolist()\n",
        "\n",
        "obj_vecs = embed_texts(objects, batch=256)\n",
        "sims = cosine_sim(obj_vecs, mat)\n",
        "top_idx = sims.argsort(axis=1)[:, ::-1][:, :5]\n",
        "top_scores = np.take_along_axis(sims, top_idx, axis=1)\n",
        "\n",
        "# Apply small path-based boost and pick best\n",
        "best_ids = []\n",
        "best_scores = []\n",
        "for i, obj in enumerate(objects):\n",
        "    cand_idxs = top_idx[i]\n",
        "    cand_scores = top_scores[i]\n",
        "    adj = [cand_scores[j] + boost_with_path(obj, texts[cand_idxs[j]]) for j in range(len(cand_idxs))]\n",
        "    j = int(np.argmax(adj))\n",
        "    best_ids.append(node_ids[cand_idxs[j]])\n",
        "    best_scores.append(float(adj[j]))\n",
        "\n",
        "df_dbg = pd.DataFrame({\"object_text\": objects, \"node_id\": best_ids, \"score\": best_scores})\n",
        "df_dbg[\"label\"] = df_dbg[\"node_id\"].map(lambda nid: label_by_id.get(nid, nid))\n",
        "df_dbg[\"uri\"]   = df_dbg[\"node_id\"].map(lambda nid: uri_by_id.get(nid, nid))\n",
        "\n",
        "# Suggest thresholds from distribution\n",
        "p50, p75, p90 = np.percentile(df_dbg[\"score\"], [50,75,90])\n",
        "suggested = {\"mid\": float(max(0.70, p50)), \"high\": float(max(0.78, p75))}\n",
        "\n",
        "df_dbg.to_csv(\"training_openai4b_mapping_preview.csv\", index=False)\n",
        "with open(\"training_openai4b_thresholds.json\", \"w\") as f:\n",
        "    json.dump({\"p50\": float(p50), \"p75\": float(p75), \"p90\": float(p90), \"suggested\": suggested}, f, indent=2)\n",
        "\n",
        "print(\"Wrote training_openai4b_mapping_preview.csv with\", len(df_dbg), \"rows\")\n",
        "print(\"Suggested thresholds:\", suggested)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c7882c",
      "metadata": {
        "id": "15c7882c"
      },
      "source": [
        "## 8) (Optional) Simple Evaluation Examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PBsY8C9YjsBR"
      },
      "id": "PBsY8C9YjsBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2297f1bd",
      "metadata": {
        "id": "2297f1bd"
      },
      "source": [
        "## 9) Save Artifacts\n",
        "\n",
        "Un-comment the save lines in Sections 4–7 after training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this to the folder you want\n",
        "FOLDER = \"/content/trained_models\"\n",
        "ARCHIVE = \"/content/trained_models.zip\"\n",
        "\n",
        "# Zip it (recursive, store paths relative to the folder’s parent)\n",
        "!cd \"$(dirname \"$FOLDER\")\" && zip -r \"$(basename \"$ARCHIVE\")\" \"$(basename \"$FOLDER\")\"\n",
        "\n",
        "from google.colab import files\n",
        "files.download(ARCHIVE)  # opens a browser download dialog"
      ],
      "metadata": {
        "id": "ZdpSxIiLbWJl"
      },
      "id": "ZdpSxIiLbWJl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}