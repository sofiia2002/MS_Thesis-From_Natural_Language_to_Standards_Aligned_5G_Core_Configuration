{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a544f0a",
      "metadata": {
        "id": "3a544f0a"
      },
      "source": [
        "\n",
        "# 02 — Ingest PDF → Turtle → AuraDB (v7: OpenAI for Step 4b)\n",
        "\n",
        "This version replaces the 4b classifier with **OpenAI embeddings** nearest-neighbor mapping.\n",
        "Set `OPENAI_API_KEY` in your env before running.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab5e6ab",
      "metadata": {
        "id": "eab5e6ab"
      },
      "source": [
        "## 0) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "781b75ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "781b75ed",
        "outputId": "cb77a42a-3d7a-476e-c013-2e7f6435c184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.12/dist-packages (7.1.4)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.12/dist-packages (5.28.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from rdflib) (3.2.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U openai PyMuPDF pandas numpy tqdm transformers sentencepiece rdflib neo4j joblib spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e518207f",
      "metadata": {
        "id": "e518207f"
      },
      "source": [
        "## 1) Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "HvtOU0K-kEt8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvtOU0K-kEt8",
        "outputId": "5d2848c3-5c77-4333-c5ec-d50064c1f2b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76be388",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "c76be388",
        "outputId": "d427e0e6-c5cb-4d3e-ca62-c251370ad80c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5cce6f43-1e0b-4d50-aa0f-dc8e87491e29\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5cce6f43-1e0b-4d50-aa0f-dc8e87491e29\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving ts_138300v180600p.pdf to ts_138300v180600p (3).pdf\n",
            "Using: /content/ts_138300v180600p (3).pdf\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # pick a PDF from your computer\n",
        "pdf_name = next(iter(uploaded))         # first selected file\n",
        "PDF_PATH = Path(pdf_name)\n",
        "print(\"Using:\", PDF_PATH.resolve())\n",
        "\n",
        "MODEL_DIR_SIMPLIFIER   = Path(\"./drive/MyDrive/MGR/trained_models/t5_simplifier_step2\")\n",
        "MODEL_DIR_TRIPLE       = Path(\"./drive/MyDrive/MGR/trained_models/t5_triple_step3\")\n",
        "MODEL_DIR_REL_CLASSIF  = Path(\"./drive/MyDrive/MGR/trained_models/distilbert_relation_step4a\")\n",
        "\n",
        "# OpenAI 4b artifacts\n",
        "OPENAI_4B_DIR = Path(\"./drive/MyDrive/MGR/trained_models/openai_class_mapper_4b\")\n",
        "TAXO_EMB_NPY  = OPENAI_4B_DIR / \"taxonomy_embeddings.npy\"\n",
        "TAXO_META_JSON= OPENAI_4B_DIR / \"taxonomy_meta.json\"\n",
        "\n",
        "LABELMAP_DIR = Path(\"./drive/MyDrive/MGR/trained_models/label_maps\")\n",
        "REL_LABEL2ID_PATH = LABELMAP_DIR / \"relation_label2id.joblib\"\n",
        "\n",
        "TAXONOMY_CSV = Path(\"./data/taxonomy.csv\")  # for URIs\n",
        "\n",
        "NEO4J_URI  = os.getenv(\"NEO4J_URI\",  \"neo4j+s://examole.databases.neo4j.io\")\n",
        "NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
        "NEO4J_PASS = os.getenv(\"NEO4J_PASS\", \"example\")\n",
        "\n",
        "BASE_NS = \"http://example.org/telecom#\"\n",
        "PROV_NS = \"http://www.w3.org/ns/prov#\"\n",
        "\n",
        "OPENAI_API_KEY=\"example\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472281e1",
      "metadata": {
        "id": "472281e1"
      },
      "source": [
        "## 2) Extract Telecom-Heavy Sentences from PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "36e73c2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36e73c2e",
        "outputId": "34aa3a90-65d6-4dd9-89e6-a8d6e5abb05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 1985 candidate sentences.\n",
            "[(1, 'ETSI TS 138 300 V18.6.0 (2025-07) \\n5G; \\nNR; \\nNR and NG-RAN Overall description; \\nStage-2  \\n(3GPP TS 38.300 version 18.6.0'), (2, 'Release 18\\n \\nReference \\nRTS/TSGR-0238300vi60 \\nKeywords \\n5G \\nETSI \\n650 Route des Lucioles \\nF-06921 Sophia Antipolis Cedex - FRANCE \\n \\nTel.:'), (2, 'Fax: +33 4 93 65 47 16 \\n \\nSiret N° 348 623 562 00017 - APE 7112B \\nAssociation à but non lucratif enregistrée à la \\nSous-Préfecture de Grasse (06) N° w061004871 \\n \\nImportant notice'), (2, 'If you find errors in the present document, please send your comments to \\nthe relevant service listed under Committee Support Staff.'), (2, 'No recommendation as to products and services or vendors is made or should be implied.')]\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "TELECOM_TRIGGERS = [\n",
        "    \"5g\", \"4g\", \"lte\", \"nr\", \"ran\", \"gNodeB\", \"ng-eNB\", \"amf\", \"smf\", \"upf\", \"ausf\",\n",
        "    \"udm\", \"pcf\", \"nrf\", \"bsf\", \"nef\", \"network slice\", \"QoS\", \"qci\", \"s1ap\",\n",
        "    \"ngap\", \"diameter\", \"sctp\", \"ims\", \"epc\", \"open5gs\", \"core network\",\n",
        "    \"pdu session\", \"mme\", \"sgw\", \"pgw\", \"slice\", \"mec\", \"edge\", \"sba\", \"n2\", \"n3\", \"n6\", \"n8\", \"n10\", \"n11\"\n",
        "]\n",
        "\n",
        "def extract_sentences_with_pages(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    sentences = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc[i]\n",
        "        text = page.get_text(\"text\")\n",
        "        for sent in nlp(text).sents:\n",
        "            s = sent.text.strip()\n",
        "            if s:\n",
        "                sentences.append((i+1, s))\n",
        "    return sentences\n",
        "\n",
        "def is_telecom_heavy(text):\n",
        "    low = text.lower()\n",
        "    return any(tok.lower() in low for tok in TELECOM_TRIGGERS)\n",
        "\n",
        "candidates = [(p, s) for (p, s) in extract_sentences_with_pages(PDF_PATH) if is_telecom_heavy(s)]\n",
        "print(f\"Extracted {len(candidates)} candidate sentences.\")\n",
        "print(candidates[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c66504",
      "metadata": {
        "id": "66c66504"
      },
      "source": [
        "## 3) Load Trained Models (2,3,4a) & Taxonomy Embeddings (OpenAI 4b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "42fc1701",
      "metadata": {
        "id": "42fc1701"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch, joblib, json, numpy as np, pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# T5 (2 & 3)\n",
        "t5_tok = T5TokenizerFast.from_pretrained(MODEL_DIR_SIMPLIFIER)\n",
        "simplifier = T5ForConditionalGeneration.from_pretrained(MODEL_DIR_SIMPLIFIER).to(device)\n",
        "\n",
        "triple_tok = T5TokenizerFast.from_pretrained(MODEL_DIR_TRIPLE)\n",
        "triple_model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR_TRIPLE).to(device)\n",
        "\n",
        "# 4a classifier\n",
        "rel_tok = AutoTokenizer.from_pretrained(MODEL_DIR_REL_CLASSIF)\n",
        "rel_model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR_REL_CLASSIF).to(device)\n",
        "\n",
        "# Taxonomy info\n",
        "taxonomy_df = pd.read_csv(TAXONOMY_CSV)\n",
        "taxo_uri = {row[\"node_id\"]: row[\"uri\"] for _, row in taxonomy_df.iterrows()}\n",
        "taxo_label = {row[\"node_id\"]: row[\"label\"] for _, row in taxonomy_df.iterrows()}\n",
        "\n",
        "# Embedding matrix + meta\n",
        "mat = np.load(TAXO_EMB_NPY)\n",
        "with open(TAXO_META_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "node_ids = meta[\"node_ids\"]\n",
        "texts = meta[\"texts\"]\n",
        "assert mat.shape[0] == len(node_ids), \"Embedding matrix vs node_ids mismatch\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89da4fd",
      "metadata": {
        "id": "a89da4fd"
      },
      "source": [
        "## 4) Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "aefb67a2",
      "metadata": {
        "id": "aefb67a2"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def t5_generate(model, tokenizer, inputs: List[str], max_len=128, prefix=None):\n",
        "    model.eval()\n",
        "    outs = []\n",
        "    for text in inputs:\n",
        "        if prefix:\n",
        "            text = prefix + text\n",
        "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(**enc, max_length=max_len, num_beams=4)\n",
        "        outs.append(tokenizer.decode(gen[0], skip_special_tokens=True).strip())\n",
        "    return outs\n",
        "\n",
        "def classify_relations(model, tokenizer, texts: List[str]) -> List[str]:\n",
        "    model.eval()\n",
        "    labels = []\n",
        "    for text in texts:\n",
        "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "        pred_id = int(torch.argmax(logits, dim=-1).cpu().item())\n",
        "        labels.append(model.config.id2label.get(pred_id, str(pred_id)))\n",
        "    return labels\n",
        "\n",
        "def embed_openai(texts: List[str], model=\"text-embedding-3-large\"):\n",
        "    # returns [n, d] numpy array\n",
        "    vecs = []\n",
        "    for i in range(0, len(texts), 256):\n",
        "        batch = texts[i:i+256]\n",
        "        emb = client.embeddings.create(model=model, input=batch)\n",
        "        vecs.extend([e.embedding for e in emb.data])\n",
        "    return np.array(vecs, dtype=\"float32\")\n",
        "\n",
        "def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-9)\n",
        "    b = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-9)\n",
        "    return a @ b.T  # [n, m]\n",
        "\n",
        "def map_to_taxonomy(objects: List[str], top_k=5, threshold=0.75):\n",
        "    obj_vecs = embed_openai(objects)\n",
        "    sims = cosine_sim(obj_vecs, mat)  # [len(objects), N_nodes]\n",
        "    top_ids = sims.argsort(axis=1)[:, ::-1][:, :top_k]\n",
        "    top_scores = np.take_along_axis(sims, top_ids, axis=1)\n",
        "    results = []\n",
        "    for i, s in enumerate(objects):\n",
        "        idxs = top_ids[i]\n",
        "        scores = top_scores[i]\n",
        "        nid = node_ids[idxs[0]]\n",
        "        results.append((s, nid, float(scores[0])))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7fbf63",
      "metadata": {
        "id": "1a7fbf63"
      },
      "source": [
        "## 5) Steps 2 & 3 — Simplify → Extract Triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f0ae7011",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ae7011",
        "outputId": "46268721-ed69-4e96-ac12-1e41e2ab24da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidates: 1985, device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 125/125 [10:43<00:00,  5.15s/it]\n",
            "Generating: 100%|██████████| 125/125 [06:15<00:00,  3.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample triples: [(2, 'ETSI', 'shallNotBeHonoreInAnyEventFor', 'AnyDaunt whatsoever'), (2, 'CopyrightNotificationNoParts', 'mayBeReproducedAndUsedIn', 'AnyForm or By Any Means'), (3, 'ETSISR 000314Updates', 'are', 'EssentialToThe PresentDokument'), (3, 'PLUGTESTSTM', 'is', 'trademarkOfETSIregisteredForTheTunnelOfSeconds'), (3, '3GPPTMLogo', 'isRegistratedFor', 'BenefitsOfSeconds')]\n",
            "Total triples parsed: 1543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Fast batched version of Steps 2 & 3\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "simplifier.to(device).eval()\n",
        "triple_model.to(device).eval()\n",
        "\n",
        "def t5_generate_batched(model, tokenizer, texts, *, max_len=128, batch_size=16, num_beams=1):\n",
        "    outs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating\", total=(len(texts)+batch_size-1)//batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tokenizer(batch, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        with torch.inference_mode():\n",
        "            if device.type == \"cuda\":\n",
        "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                    gen = model.generate(**enc, max_length=max_len, num_beams=num_beams)\n",
        "            else:\n",
        "                gen = model.generate(**enc, max_length=max_len, num_beams=num_beams)\n",
        "        outs.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
        "    return [o.strip() for o in outs]\n",
        "\n",
        "# (Optional) thin out candidates to test speed first\n",
        "# candidates = candidates[:200]\n",
        "\n",
        "pages, texts0 = zip(*candidates) if candidates else ([], [])\n",
        "print(f\"Candidates: {len(texts0)}, device: {device}\")\n",
        "\n",
        "# Step 2 (complex -> simple): shorter outputs, no beam search\n",
        "simple_sentences = t5_generate_batched(\n",
        "    simplifier, t5_tok, list(texts0),\n",
        "    max_len=96, batch_size=16, num_beams=1\n",
        ")\n",
        "\n",
        "# Step 3 (simple -> triple): keep short outputs, no beam search\n",
        "triple_strings = t5_generate_batched(\n",
        "    triple_model, triple_tok, list(simple_sentences),\n",
        "    max_len=48, batch_size=16, num_beams=1\n",
        ")\n",
        "\n",
        "def parse_triple(s):\n",
        "    parts = [p.strip() for p in s.split(\"|\")]\n",
        "    if len(parts) != 3: return None, None, None\n",
        "    return parts[0], parts[1], parts[2]\n",
        "\n",
        "triples_raw = []\n",
        "for p, t in zip(pages, triple_strings):\n",
        "    a, r, b = parse_triple(t)\n",
        "    if a and r and b:\n",
        "        triples_raw.append((p, a, r, b))\n",
        "\n",
        "print(\"Sample triples:\", triples_raw[:5])\n",
        "print(f\"Total triples parsed: {len(triples_raw)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b939ec2",
      "metadata": {
        "id": "0b939ec2"
      },
      "source": [
        "## 6) Step 4a — Normalize Relations (DistilBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d102eee1",
      "metadata": {
        "id": "d102eee1"
      },
      "outputs": [],
      "source": [
        "raw_relations = [r for _, _, r, _ in triples_raw]\n",
        "norm_rel = classify_relations(rel_model, rel_tok, raw_relations)\n",
        "\n",
        "def rel_to_iri(label: str) -> str:\n",
        "    safe = \"\".join(ch if ch.isalnum() else \"_\" for ch in label).strip(\"_\")\n",
        "    return f\"{BASE_NS}{safe}\"\n",
        "rel_iris = [rel_to_iri(x) for x in norm_rel]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab956ac",
      "metadata": {
        "id": "8ab956ac"
      },
      "source": [
        "## 7) Step 4b — Map Classes via OpenAI Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dbd172ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbd172ec",
        "outputId": "bec86c12-3218-434c-806c-02fae0ca0b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample normalized triples: [(2, 'http://example.org/telecom#EAP5G', 'http://example.org/telecom#isusedfor', 'http://example.org/telecom#AV'), (2, 'http://example.org/telecom#Privacy', 'http://example.org/telecom#isusedin', 'http://example.org/telecom#FormOfNon3GPPAccess'), (3, 'http://example.org/telecom#Release19TR28914', 'http://example.org/telecom#are', 'http://example.org/telecom#EssentialInPrivate5GEnvironments'), (3, 'http://example.org/telecom#Testbed', 'http://example.org/telecom#is', 'http://example.org/telecom#IPsecTunnelingForNon3GPPAccessPaths'), (3, 'http://example.org/telecom#3GPPTS29244', 'http://example.org/telecom#isusedfor', 'http://example.org/telecom#MetricsEndpointsEvery10Seconds')]\n",
            "Wrote mapping debug report to debug_class_mapping.csv\n"
          ]
        }
      ],
      "source": [
        "# --- Robust 4b mapping with top-k ranking, path boosting, thresholding, and optional GPT tie-break ---\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "TOP_K = 5\n",
        "HIGH_THRESH = 0.78       # accept confidently\n",
        "LOW_THRESH  = 0.70       # below this, still take best but mark low_confidence\n",
        "USE_GPT_FOR_TIEBREAK = True  # requires OPENAI_API_KEY and internet access\n",
        "\n",
        "# Build a label lookup for leaf text (strip path) to node_id for light heuristics\n",
        "leaf_text_by_id = {nid: txt.split(\"⟂\")[0].strip() if \"⟂\" in txt else txt for nid, txt in zip(node_ids, texts)}\n",
        "\n",
        "def rank_taxonomy(objects, top_k=TOP_K):\n",
        "    # Use the helpers from section 4 (embed_openai, cosine_sim, 'mat', 'node_ids', 'texts')\n",
        "    vecs = embed_openai(objects)\n",
        "    sims = cosine_sim(vecs, mat)\n",
        "    top_ids = sims.argsort(axis=1)[:, ::-1][:, :top_k]\n",
        "    top_scores = np.take_along_axis(sims, top_ids, axis=1)\n",
        "    return top_ids, top_scores  # indices into node_ids\n",
        "\n",
        "def boost_with_path(object_str, node_text):\n",
        "    # Small boost if object tokens appear in the candidate's path text\n",
        "    s = object_str.lower()\n",
        "    t = node_text.lower()\n",
        "    score = 0.0\n",
        "    # Exact leaf match gets a bigger boost\n",
        "    leaf = node_text.split(\"⟂\")[0].strip().lower() if \"⟂\" in node_text else node_text.lower()\n",
        "    if s == leaf:\n",
        "        score += 0.06\n",
        "    # Token overlap\n",
        "    toks = [w for w in re.findall(r\"[a-zA-Z0-9]+\", s) if len(w) > 2]\n",
        "    overlap = sum(1 for w in toks if w in t)\n",
        "    if overlap:\n",
        "        score += min(0.04, 0.01 * overlap)\n",
        "    return score\n",
        "\n",
        "def pick_best(object_str, cand_node_idxs, cand_scores):\n",
        "    # Apply a tiny path-based boost before selecting\n",
        "    adj_scores = []\n",
        "    for idx, base in zip(cand_node_idxs, cand_scores):\n",
        "        nid  = node_ids[idx]\n",
        "        text = texts[idx]\n",
        "        adj = base + boost_with_path(object_str, text)\n",
        "        adj_scores.append(adj)\n",
        "    # Return best by adjusted score\n",
        "    best_pos = int(np.argmax(adj_scores))\n",
        "    return cand_node_idxs[best_pos], float(adj_scores[best_pos])\n",
        "\n",
        "def gpt_tiebreak(object_str, cand_node_idxs):\n",
        "    # Ask GPT to choose the best among top-3 when confidence is marginal\n",
        "    if not USE_GPT_FOR_TIEBREAK:\n",
        "        return cand_node_idxs[0]\n",
        "    try:\n",
        "        options = [\n",
        "            {\"id\": node_ids[i], \"label\": taxo_label.get(node_ids[i], node_ids[i]), \"path\": texts[i]}\n",
        "            for i in cand_node_idxs[:3]\n",
        "        ]\n",
        "        prompt = (\n",
        "            \"Select the single best taxonomy class for the object.\\n\"\n",
        "            f\"Object: {object_str}\\n\\n\"\n",
        "            \"Candidates:\\n\" +\n",
        "            \"\\n\".join([f\"- id={o['id']} | label={o['label']} | path={o['path']}\" for o in options]) +\n",
        "            \"\\n\\nReturn only the id of the best candidate.\"\n",
        "        )\n",
        "        rsp = client.responses.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            input=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        txt = rsp.output_text.strip()\n",
        "        # Extract the id (must match one of the candidates)\n",
        "        for o in options:\n",
        "            if o[\"id\"] in txt:\n",
        "                return o[\"id\"]\n",
        "        # If not found, fallback to the first\n",
        "        return options[0][\"id\"]\n",
        "    except Exception as e:\n",
        "        # If API call fails, silently fallback to first\n",
        "        return node_ids[cand_node_idxs[0]]\n",
        "\n",
        "def map_objects(objects):\n",
        "    top_ids, top_scores = rank_taxonomy(objects, top_k=TOP_K)\n",
        "    picks = []\n",
        "    for obj, idxs, scores in zip(objects, top_ids, top_scores):\n",
        "        best_idx, best_score = pick_best(obj, idxs, scores)\n",
        "        nid = node_ids[best_idx]\n",
        "        conf = \"high\" if best_score >= HIGH_THRESH else (\"mid\" if best_score >= LOW_THRESH else \"low\")\n",
        "        # Tie-break with GPT for mid-confidence\n",
        "        if conf == \"mid\":\n",
        "            chosen_id = gpt_tiebreak(obj, idxs)\n",
        "            if chosen_id != nid:\n",
        "                # If GPT picked a different id, recompute score from our sims array\n",
        "                chosen_pos = list(idxs).index(node_ids.index(chosen_id)) if chosen_id in node_ids else 0\n",
        "                nid = chosen_id\n",
        "        picks.append((obj, nid, float(best_score), conf))\n",
        "    return picks\n",
        "\n",
        "# Run mapping\n",
        "raw_a = [a for _, a, _, _ in triples_raw]\n",
        "raw_b = [b for _, _, _, b in triples_raw]\n",
        "\n",
        "mapped_a = map_objects(raw_a)\n",
        "mapped_b = map_objects(raw_b)\n",
        "\n",
        "# Build URIs and a small debug frame\n",
        "def to_uri(node_id: str) -> str:\n",
        "    return taxo_uri.get(node_id, f\"{BASE_NS}{node_id}\")\n",
        "\n",
        "a_uris = [to_uri(nid) for (_, nid, score, conf) in mapped_a]\n",
        "b_uris = [to_uri(nid) for (_, nid, score, conf) in mapped_b]\n",
        "\n",
        "normalized = [(p, sa, pr, ob) for (p, sa, pr, ob) in zip([t[0] for t in triples_raw], a_uris, rel_iris, b_uris)]\n",
        "print(\"Sample normalized triples:\", normalized[:5])\n",
        "\n",
        "# Write a debug report to inspect mapping quality\n",
        "dbg_a = pd.DataFrame(mapped_a, columns=[\"object_text\",\"node_id\",\"score\",\"confidence\"]).assign(role=\"subject\")\n",
        "dbg_b = pd.DataFrame(mapped_b, columns=[\"object_text\",\"node_id\",\"score\",\"confidence\"]).assign(role=\"object\")\n",
        "dbg = pd.concat([dbg_a, dbg_b], ignore_index=True)\n",
        "dbg[\"label\"] = dbg[\"node_id\"].map(lambda nid: taxo_label.get(nid, nid))\n",
        "dbg.to_csv(\"debug_class_mapping.csv\", index=False)\n",
        "print(\"Wrote mapping debug report to debug_class_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08ed99b3",
      "metadata": {
        "id": "08ed99b3"
      },
      "source": [
        "## 8) Build Turtle & Load into AuraDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f487c510",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f487c510",
        "outputId": "cbf5cd8d-2a4d-489f-bb82-1a0394214827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote: /content/output.ttl\n",
            "n10s not available or failed; falling back. Details: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `n10s.graphconfig.show` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1446/1446 [03:40<00:00,  6.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fallback MERGE completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
        "from neo4j import GraphDatabase\n",
        "from rdflib import URIRef as RDFURIRef\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "g = Graph()\n",
        "BASE = Namespace(BASE_NS)\n",
        "PROV = Namespace(PROV_NS)\n",
        "g.bind(\"base\", BASE)\n",
        "g.bind(\"prov\", PROV)\n",
        "\n",
        "# GOOD: percent-encodes spaces, parentheses, etc.\n",
        "source_uri = URIRef(Path(PDF_PATH).resolve().as_uri())\n",
        "\n",
        "for page, s_uri, p_iri, o_uri in normalized:\n",
        "    s = URIRef(s_uri); p = URIRef(p_iri); o = URIRef(o_uri)\n",
        "    g.add((s, p, o))\n",
        "    bn = BNode()\n",
        "    g.add((bn, PROV.wasDerivedFrom, source_uri))\n",
        "    g.add((bn, PROV.value, Literal(int(page))))\n",
        "    g.add((bn, PROV.wasGeneratedBy, URIRef(p_iri)))\n",
        "\n",
        "TTL_PATH = Path(\"./output.ttl\")\n",
        "g.serialize(destination=str(TTL_PATH), format=\"turtle\")\n",
        "print(\"Wrote:\", TTL_PATH.resolve())\n",
        "\n",
        "def try_n10s_import(uri, user, password, ttl_text):\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    with driver.session() as sess:\n",
        "        try:\n",
        "            _ = sess.run(\"CALL n10s.graphconfig.show()\").data()\n",
        "            res = sess.run(\"CALL n10s.rdf.import.inline($ttl, 'Turtle') YIELD terminationStatus, triplesLoaded, triplesParsed\",\n",
        "                           ttl=ttl_text).data()\n",
        "            return True, res\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "        finally:\n",
        "            driver.close()\n",
        "\n",
        "def sanitize_rel_type(iri_tail: str) -> str:\n",
        "    out = \"\".join(ch if ch.isalnum() else \"_\" for ch in iri_tail)\n",
        "    if not out: out = \"RELATED_TO\"\n",
        "    if out[0].isdigit(): out = \"_\" + out\n",
        "    return out.upper()\n",
        "\n",
        "def iri_tail(iri: str) -> str:\n",
        "    for sep in [\"#\", \"/\", \":\"]:\n",
        "        if sep in iri:\n",
        "            iri = iri.rsplit(sep, 1)[-1]\n",
        "    return iri\n",
        "\n",
        "def fallback_merge(uri, user, password, ttl_path):\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    gg = Graph(); gg.parse(ttl_path, format=\"turtle\")\n",
        "    triples = [(str(s), str(p), str(o)) for (s, p, o) in gg if isinstance(s, RDFURIRef) and isinstance(o, RDFURIRef)]\n",
        "    with driver.session() as sess:\n",
        "        sess.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:Resource) REQUIRE n.uri IS UNIQUE\")\n",
        "        for s, p, o in tqdm(triples):\n",
        "            rel_type = sanitize_rel_type(iri_tail(p))\n",
        "            q = \"MERGE (s:Resource {uri:$suri})\\n\" +                 \"MERGE (o:Resource {uri:$ouri})\\n\" +                 f\"MERGE (s)-[r:{rel_type}]->(o)\"\n",
        "            sess.run(q, suri=s, ouri=o)\n",
        "    driver.close()\n",
        "\n",
        "ttl_text = Path(TTL_PATH).read_text(encoding=\"utf-8\")\n",
        "ok, info = try_n10s_import(NEO4J_URI, NEO4J_USER, NEO4J_PASS, ttl_text)\n",
        "if ok:\n",
        "    print(\"n10s import finished:\", info)\n",
        "else:\n",
        "    print(\"n10s not available or failed; falling back. Details:\", str(info)[:800])\n",
        "    fallback_merge(NEO4J_URI, NEO4J_USER, NEO4J_PASS, TTL_PATH)\n",
        "    print(\"Fallback MERGE completed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
